{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b302172a",
   "metadata": {},
   "source": [
    "# Hybrid NF-MCMC Demo - Algorithm 2: On-the-fly NF\n",
    "### Run script to run example Algorithm 2 experiment for dV = 0\n",
    "\n",
    "Total script run time ~15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44252419",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from scipy.spatial.distance import squareform, cdist\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from demo_utils import get_project_root\n",
    "# Add the custom path for hybrid_nf_mcmc\n",
    "project_root = get_project_root()\n",
    "hybrid_nf_mcmc_path = os.path.join(project_root, \"hybrid_NF_MCMC\")\n",
    "sys.path.append(hybrid_nf_mcmc_path)\n",
    "from utils import (get_project_root, setup_logger, get_dataloader, calculate_well_statistics, \n",
    "                  set_icl_color_cycle, get_icl_heatmap_cmap, plot_loss, \n",
    "                  plot_frequency_heatmap, generate_samples, \n",
    "                  calculate_pair_correlation, plot_pair_correlation, save_rdf_data, plot_acceptance_rate,\n",
    "                  plot_avg_free_energy, plot_well_statistics, plot_avg_x_coordinate,\n",
    "                  plot_multiple_avg_x_coordinates)\n",
    "import argparse\n",
    "import csv\n",
    "\n",
    "set_icl_color_cycle()\n",
    "cmap_div = get_icl_heatmap_cmap(\"diverging\")\n",
    "\n",
    "# importing the NF and MC modules\n",
    "project_root = get_project_root()\n",
    "nf_path = os.path.join(project_root, \"NF\")\n",
    "sys.path.append(nf_path)\n",
    "sys.path.append(project_root)\n",
    "import normflows as NF\n",
    "import MCMC as MC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c19e895",
   "metadata": {},
   "source": [
    "Parameter inputs\n",
    "\n",
    "To increase accuracy and precision run for the number of training cycles and with number of parallel MCMC runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec124169",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'A2_demo'\n",
    "\n",
    "# define experiment variables \n",
    "NUM_MC_RUNS = 10        # how many mc runs going on in 'parallel'\n",
    "MASTER_SEED = 42        # master seed to give seeds for each mc run to run differently\n",
    "NUM_PARTICLES = 3\n",
    "NUM_DIM = 2\n",
    "NUM_TRAINING_CYCLES = 50            # how many times NF is updated and Global moves are preformed \n",
    "NF_TARGET_ACC = 0.5\n",
    "\n",
    "# nf needed params\n",
    "INITIAL_TRAINING_NUM_SAMPLES = 1000\n",
    "BATCH_SIZE = 256\n",
    "K = 23\n",
    "EPOCHS = 1\n",
    "LR = 0.000543510751759681\n",
    "WEIGHT_DECAY = 9.5857178422352e-05\n",
    "UPDATE_NUM_SAMPLES = 1000\n",
    "CHECKPOINT_INTERVAL = 10\n",
    "HIDDEN_UNITS = 128\n",
    "NUM_BINS = 15\n",
    "N_BLOCKS = 2\n",
    "ALPHA = 1.0\n",
    "\n",
    "# mc needed params\n",
    "TEMP = 1.0\n",
    "RHO = 0.03\n",
    "ASPECT_RATIO = 1.0\n",
    "VISUALISE = True\n",
    "CHECKING = True\n",
    "EQUILIBRATION_STEPS = 5000\n",
    "NUM_WELLS = 2\n",
    "V0_LIST = [-10.0,-10.0]              # depth of wells - adjust to change the dV\n",
    "R0 = 1.2                \n",
    "K_VAL = 15              \n",
    "HALF_BOX = ((NUM_PARTICLES/RHO)**(1/NUM_DIM))/2\n",
    "INITIAL_MAX_DISPLACEMENT = 0.65\n",
    "SAMPLING_FREQUENCY = 10\n",
    "ADJUSTING_FREQUENCY = 100000\n",
    "CUMULATIVE_TRAINING_SAMPLES = False # False: we only the new samples for each training round\n",
    "PRODUCTION_SAMPLES = int((UPDATE_NUM_SAMPLES / (NUM_MC_RUNS / SAMPLING_FREQUENCY)) / SAMPLING_FREQUENCY) * NUM_TRAINING_CYCLES\n",
    "\n",
    "# analysis parameters\n",
    "NUM_SAMPLES_FOR_ANALYSIS = 100 * NUM_MC_RUNS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32f58d",
   "metadata": {},
   "source": [
    "Saving parameters, setting up results directory and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5395a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(project_root, \"demos/data\", experiment_name)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "# Setup a global experiment logger that logs to a file in the experiment directory.\n",
    "experiment_log_file = os.path.join(directory, \"experiment.log\")\n",
    "experiment_logger = setup_logger(\"experiment\", experiment_log_file, file_level=logging.DEBUG, stream_level=logging.INFO)\n",
    "experiment_logger.info(\"half box is: \" + str(HALF_BOX))\n",
    "experiment_logger.info(f\"Directory created at: {directory}\")\n",
    "\n",
    "mc_runs_directory = os.path.join(directory, \"mc_runs\")\n",
    "os.makedirs(mc_runs_directory, exist_ok=True)\n",
    "training_directory = os.path.join(directory, \"training\")\n",
    "os.makedirs(training_directory, exist_ok=True)\n",
    "\n",
    "# Save experiment parameters to a JSON file in the parent experiment directory\n",
    "experiment_params = {\n",
    "    \"NUM_MC_RUNS\": NUM_MC_RUNS,\n",
    "    \"MASTER_SEED\": MASTER_SEED,\n",
    "    \"NUM_PARTICLES\": NUM_PARTICLES,\n",
    "    \"NUM_DIM\": NUM_DIM,\n",
    "    \"NUM_TRAINING_CYCLES\": NUM_TRAINING_CYCLES,\n",
    "    \"INITIAL_TRAINING_NUM_SAMPLES\": INITIAL_TRAINING_NUM_SAMPLES,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"K\": K,\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"LR\": LR,\n",
    "    \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
    "    \"UPDATE_NUM_SAMPLES\": UPDATE_NUM_SAMPLES,\n",
    "    \"HIDDEN_UNITS\": HIDDEN_UNITS,\n",
    "    \"NUM_BINS\": NUM_BINS,\n",
    "    \"N_BLOCKS\": N_BLOCKS,\n",
    "    \"ALPHA\": ALPHA,\n",
    "    \"TEMP\": TEMP,\n",
    "    \"RHO\": RHO,\n",
    "    \"ASPECT_RATIO\": ASPECT_RATIO,\n",
    "    \"VISUALISE\": VISUALISE,\n",
    "    \"CHECKING\": CHECKING,\n",
    "    \"EQUILIBRATION_STEPS\": EQUILIBRATION_STEPS,\n",
    "    \"NUM_WELLS\": NUM_WELLS,\n",
    "    \"V0_LIST\": V0_LIST,\n",
    "    \"R0\": R0,\n",
    "    \"K_VAL\": K_VAL,\n",
    "    \"HALF_BOX\": HALF_BOX,\n",
    "    \"INITIAL_MAX_DISPLACEMENT\": INITIAL_MAX_DISPLACEMENT,\n",
    "    \"SAMPLING_FREQUENCY\": SAMPLING_FREQUENCY,\n",
    "    \"ADJUSTING_FREQUENCY\": ADJUSTING_FREQUENCY,\n",
    "    \"CUMULATIVE_TRAINING_SAMPLES\": CUMULATIVE_TRAINING_SAMPLES,\n",
    "    \"NUM_SAMPLES_FOR_ANALYSIS\": NUM_SAMPLES_FOR_ANALYSIS\n",
    "}\n",
    "json_file_path = os.path.join(directory, \"params.json\")\n",
    "with open(json_file_path, \"w\") as f:\n",
    "    json.dump(experiment_params, f, indent=4)\n",
    "experiment_logger.info(f\"Experiment parameters saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c6f99",
   "metadata": {},
   "source": [
    "Experiment: on-the-fly training and testing \n",
    "\n",
    "Run-time: ~ 10 mins of training on M1 chip (no CUDA)\n",
    "\n",
    "Check output results folder for the results: there are NF model output plots every training checkpoint interval (editable in parameters) and when the experiment ends plots are made for the stats of each individual hybrid sampler (within the MCMC runs) as well as for the whole experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b300b-fe15-49f7-8293-f780bbfb8a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation of experiment - sets up the MCMC runs and equilibrates and collects samples\n",
    "mc_runs = []  \n",
    "\n",
    "# Initialise MCMC step counter and acceptance tracking \n",
    "total_mcmc_steps = 0\n",
    "p_acc_history = [0.0]  \n",
    "mcmc_steps_history = [0]  \n",
    "big_move_attempts = 0\n",
    "big_move_accepts = 0\n",
    "\n",
    "for i in range(NUM_MC_RUNS):\n",
    "    seed = i + MASTER_SEED\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create a dedicated folder and logger for this Monte Carlo run.\n",
    "    run_dir = os.path.join(mc_runs_directory, f\"run_{i+1:03d}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    mc_log_file = os.path.join(run_dir, \"mc_run.log\")\n",
    "    mc_logger = setup_logger(f\"MC_run_{i+1:03d}\", mc_log_file, file_level=logging.DEBUG, stream_level=logging.WARNING)\n",
    "\n",
    "    # even i runs initialise left and odd initialise right\n",
    "    if i % 2 == 0:\n",
    "        particles, sim_box = MC.initialise_low_left(\n",
    "                num_particles=NUM_PARTICLES,\n",
    "                rho=RHO,\n",
    "                aspect_ratio=ASPECT_RATIO,\n",
    "                visualise=False,\n",
    "                checking=False\n",
    "            )\n",
    "        experiment_logger.info(f\"run {i} starts in left\")\n",
    "    else:\n",
    "        particles, sim_box = MC.initialise_low_right(\n",
    "                num_particles=NUM_PARTICLES,\n",
    "                rho=RHO,\n",
    "                aspect_ratio=ASPECT_RATIO,\n",
    "                visualise=False,\n",
    "                checking=False\n",
    "            )\n",
    "        experiment_logger.info(f\"run {i} starts in right\")\n",
    "        \n",
    "    mc_run = MC.MonteCarlo(\n",
    "        particles=particles,\n",
    "        sim_box=sim_box,\n",
    "        temperature=TEMP,\n",
    "        num_particles=NUM_PARTICLES,\n",
    "        num_wells=NUM_WELLS,    # Pass number of wells\n",
    "        V0_list=V0_LIST,                  # Pass well depth\n",
    "        r0=R0,\n",
    "        k=K_VAL,\n",
    "        initial_max_displacement=INITIAL_MAX_DISPLACEMENT,  # Adjust as needed\n",
    "        target_acceptance=0.5,\n",
    "        timing=False,\n",
    "        checking=CHECKING,\n",
    "        logger=mc_logger,       # Each run now logs to its own file\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    mc_runs.append(mc_run)\n",
    "    experiment_logger.info(f\"Monte Carlo run {i} initialised and stored\")\n",
    "\n",
    "# Plot the potential wells for visualization\n",
    "MC.visualise.plot_potential(\n",
    "    box_siz_x=sim_box.box_size_x,\n",
    "    box_size_y=sim_box.box_size_y,\n",
    "    V0_list=V0_LIST,\n",
    "    r0=R0,\n",
    "    k=K_VAL,\n",
    "    num_wells=NUM_WELLS,\n",
    "    output_path=directory\n",
    ")\n",
    "\n",
    "experiment_logger.info(\"All Monte Carlo runs initialised\")\n",
    "experiment_logger.info(\"list of run instances: \" + str(mc_runs))\n",
    "\n",
    "# equilibration of each mc run\n",
    "for mc_run in mc_runs:\n",
    "    for step in range(1, EQUILIBRATION_STEPS + 1):\n",
    "        mc_run.particle_displacement()\n",
    "        if step % ADJUSTING_FREQUENCY == 0:\n",
    "            mc_run.adjust_displacement()\n",
    "        if step % SAMPLING_FREQUENCY == 0:\n",
    "            sample = mc_run.sample(step)\n",
    "            mc_run.local_samples.append(sample)\n",
    "    \n",
    "# Plot and save the equilibrated configuration for each MC run\n",
    "for i, mc_run in enumerate(mc_runs):\n",
    "    run_dir = os.path.join(mc_runs_directory, f\"run_{i+1:03d}\")\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    current_config = mc_run.particles\n",
    "    plt.scatter(current_config[:, 0], current_config[:, 1], alpha=0.6)\n",
    "    plt.xlim(0, 2*HALF_BOX)\n",
    "    plt.ylim(0, 2*HALF_BOX)\n",
    "    plt.xlabel(r'$x$')\n",
    "    plt.ylabel(r'$y$') \n",
    "    plt.title(f'Equilibrated Configuration - MC Run {i+1}')\n",
    "    \n",
    "    # Save plot to the MC run's directory\n",
    "    config_plot_path = os.path.join(run_dir, f\"equilibrated_config.png\")\n",
    "    fig.savefig(config_plot_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    mc_run.logger.info(f\"Equilibrated configuration plot saved to: {config_plot_path}\")\n",
    "            \n",
    "# collect initial set of training samples\n",
    "global_samples_mc = []  # storage of mc samples in mc box range of 0,10\n",
    "production_runs = int(INITIAL_TRAINING_NUM_SAMPLES/(NUM_MC_RUNS / SAMPLING_FREQUENCY))\n",
    "experiment_logger.info(\"production runs per cycle: \" + str(production_runs))\n",
    "for mc_run in mc_runs:\n",
    "    for step in range(1, production_runs + 1):\n",
    "        mc_run.particle_displacement()\n",
    "        total_mcmc_steps += 1  \n",
    "        if step % ADJUSTING_FREQUENCY == 0:\n",
    "            mc_run.adjust_displacement()\n",
    "        if step % SAMPLING_FREQUENCY == 0:\n",
    "            sample = mc_run.sample(step)\n",
    "            mc_run.local_samples.append(sample)\n",
    "            global_samples_mc.append(sample[6])  # only config being stored (in MC box bound)\n",
    "\n",
    "global_samples_nf = np.array([np.array([particle - np.array([HALF_BOX, HALF_BOX]) for particle in config]) for config in global_samples_mc])\n",
    "\n",
    "# Initialize cumulative training samples if enabled.\n",
    "# Note that global_samples_nf were collected during the production phase of initial training.\n",
    "if CUMULATIVE_TRAINING_SAMPLES:\n",
    "    cumulative_training_samples_nf = global_samples_nf.copy()\n",
    "else:\n",
    "    # Even when not using cumulative samples for training, we still need to track all samples \n",
    "    # for the sliding window approach\n",
    "    all_collected_samples_nf = global_samples_nf.copy()\n",
    "\n",
    "# prepping initial samples as for dataloader\n",
    "enable_cuda = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n",
    "experiment_logger.info('device is: ' + str(device))\n",
    "\n",
    "unique_data = np.unique(global_samples_nf, axis=0)\n",
    "experiment_logger.info(\"Total unique samples: \" + str(unique_data.shape[0]))\n",
    "\n",
    "dataloader = get_dataloader(global_samples_nf, NUM_PARTICLES, NUM_DIM, device, batch_size=BATCH_SIZE, shuffle=True)\n",
    "experiment_logger.info(\"DataLoader details:\")\n",
    "experiment_logger.info(\"Dataset size: \" + str(len(dataloader.dataset)))\n",
    "experiment_logger.info(\"Number of batches: \" + str(len(dataloader)))\n",
    "first_batch = next(iter(dataloader))\n",
    "experiment_logger.info(\"Shape of data in first batch: \" + str(first_batch[0].shape))\n",
    "\n",
    "# initialise the normalizing flow model\n",
    "bound = HALF_BOX\n",
    "base = NF.Energy.UniformParticle(NUM_PARTICLES, NUM_DIM, bound, device=device)\n",
    "target = NF.Energy.DoubleWellLJ(NUM_PARTICLES * NUM_DIM, NUM_PARTICLES, TEMP, bound,\n",
    "                               V0_list=V0_LIST, \n",
    "                               r0=R0,               \n",
    "                               k=K_VAL)  \n",
    "flow_layers = []\n",
    "for i in range(K):\n",
    "    flow_layers += [NF.flows.CircularCoupledRationalQuadraticSpline(\n",
    "        NUM_PARTICLES * NUM_DIM, \n",
    "        N_BLOCKS, \n",
    "        HIDDEN_UNITS,\n",
    "        range(NUM_PARTICLES * NUM_DIM), \n",
    "        num_bins=NUM_BINS, \n",
    "        tail_bound=bound)] \n",
    "model = NF.NormalizingFlow(base, flow_layers, target).to(device)\n",
    "experiment_logger.info(f'Model prepared with {NUM_PARTICLES} particles and {NUM_DIM} dimensions!')\n",
    "\n",
    "# Create a directory for training results instead of per-cycle directories\n",
    "training_directory = os.path.join(directory, \"training\")\n",
    "os.makedirs(training_directory, exist_ok=True)\n",
    "training_log_file = os.path.join(training_directory, \"nf_training.log\")\n",
    "training_logger = setup_logger(\"NF_training\", training_log_file, file_level=logging.DEBUG, stream_level=logging.WARNING)\n",
    "\n",
    "# Initialize arrays to store loss history across all cycles\n",
    "cumulative_loss_history = []\n",
    "\n",
    "# Initial training\n",
    "loss_epoch = []\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "experiment_logger.info(f'Optimizer prepared with learning rate: {LR} and weight decay: {WEIGHT_DECAY}')\n",
    "data_save = []\n",
    "\n",
    "for it in trange(EPOCHS, desc=\"Training Progress\"):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # energy_loss,z = model.reverse_kld(BATCH_SIZE) -----> use if you want to test mixed training loss (requires CUDA)\n",
    "        sample_loss = model.forward_kld(batch[0].to(device))\n",
    "        # loss = ALPHA * sample_loss + (1 - ALPHA) * energy_loss -----> use if you want to test mixed training loss \n",
    "        loss = sample_loss\n",
    "\n",
    "        # Do backprop and optimizer step\n",
    "        if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "        # Log loss\n",
    "        epoch_loss += loss.item()\n",
    "    loss_epoch.append(epoch_loss / len(dataloader))\n",
    "    training_logger.info(f'Epoch {it+1}/{EPOCHS}, Loss: {loss_epoch[-1]:.4f}')\n",
    "\n",
    "# Update the loss_epoch array to store initial training losses\n",
    "cumulative_loss_history.extend(loss_epoch)\n",
    "\n",
    "# Plot loss using utility function\n",
    "loss_plot_path_svg, loss_plot_path_png = plot_loss(loss_epoch, training_directory + \"/\")\n",
    "training_logger.info(f\"Loss plot saved successfully to: {loss_plot_path_svg} and {loss_plot_path_png}\")\n",
    "\n",
    "model_path = os.path.join(training_directory, \"initial_model_circularspline_res_dense.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "training_logger.info(f\"Model saved successfully to: {model_path}\")\n",
    "\n",
    "# change to eval mode to test the NF model\n",
    "model.eval()\n",
    "samples_for_mc = []\n",
    "z = model.sample(NUM_MC_RUNS)\n",
    "z_ = z.reshape(-1, NUM_PARTICLES, NUM_DIM)  # Reshape as required\n",
    "z_ = z_.to('cpu').detach().numpy()  # Move to CPU and convert to NumPy\n",
    "z_ = z_ + HALF_BOX  # Add HALF_BOX to each element in the samples\n",
    "print(z_)\n",
    "\n",
    "# Generate samples using the utility function\n",
    "a_ = generate_samples(model, NUM_PARTICLES, NUM_DIM, \n",
    "                     n_iterations=NUM_SAMPLES_FOR_ANALYSIS // 5000 + 1, \n",
    "                     samples_per_iteration=5000)\n",
    "a_ = a_ + HALF_BOX  # Add HALF_BOX to each element in the samples\n",
    "print(a_.shape)\n",
    "samples_path = os.path.join(training_directory, \"samples.npy\")\n",
    "np.save(samples_path, a_)\n",
    "\n",
    "# Create heatmap using utility function\n",
    "heatmap_path_svg, heatmap_path_png = plot_frequency_heatmap(\n",
    "    a_ - HALF_BOX,  # Convert back to centered coordinates for plotting\n",
    "    training_directory + \"/\", \n",
    "    cmap_div, \n",
    "    HALF_BOX\n",
    ")\n",
    "print(f\"Frequency heatmap saved successfully to: {heatmap_path_svg} and {heatmap_path_png}\")\n",
    "\n",
    "# Calculate and plot pair correlation function\n",
    "r_vals, g_r = calculate_pair_correlation(a_ - HALF_BOX, NUM_PARTICLES, HALF_BOX, dr=HALF_BOX/50)\n",
    "pair_corr_path_svg, pair_corr_path_png = plot_pair_correlation(\n",
    "    r_vals, \n",
    "    g_r, \n",
    "    training_directory + \"/\"\n",
    ")\n",
    "print(f\"Pair correlation function plot saved successfully to: {pair_corr_path_svg} and {pair_corr_path_png}\")\n",
    "\n",
    "# Initialise acceptance counters\n",
    "nf_mc_accept = 0\n",
    "nf_to_mc_attempts = 0\n",
    "\n",
    "# Compute and log the acceptance probability\n",
    "p_acc = nf_mc_accept / nf_to_mc_attempts if nf_to_mc_attempts > 0 else 0\n",
    "p_acc_history.append(p_acc)\n",
    "mcmc_steps_history.append(total_mcmc_steps)\n",
    "training_logger.info(f\"Monte Carlo acceptance: {nf_mc_accept} out of {nf_to_mc_attempts} attempts (p_accept = {p_acc:.4f})\")\n",
    "\n",
    "# -------------------------\n",
    "# Begin update training cycles\n",
    "# -------------------------\n",
    "for cycle in range(NUM_TRAINING_CYCLES):\n",
    "    experiment_logger.info(f\"Starting update training cycle {cycle+1}/{NUM_TRAINING_CYCLES}\")\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Production Phase: Generate new MC samples for NF update training\n",
    "    # -------------------------------------------------------------------\n",
    "    global_samples_mc_update = []  # reset samples\n",
    "    # calculate number of production runs based on UPDATE_NUM_SAMPLES\n",
    "    production_runs = int(UPDATE_NUM_SAMPLES / (NUM_MC_RUNS / SAMPLING_FREQUENCY))\n",
    "    experiment_logger.info(f\"Cycle {cycle+1}: production runs per MC run: {production_runs}\")\n",
    "    for mc_run in mc_runs:\n",
    "        for step in range(1, production_runs + 1):\n",
    "            mc_run.particle_displacement()\n",
    "            total_mcmc_steps += 1  # Count each MCMC step\n",
    "            if step % ADJUSTING_FREQUENCY == 0:\n",
    "                mc_run.adjust_displacement()\n",
    "            if step % SAMPLING_FREQUENCY == 0:\n",
    "                sample = mc_run.sample(step)\n",
    "                mc_run.local_samples.append(sample)\n",
    "                mc_run.testing_samples.append(sample[6]) \n",
    "                # store the configuration (in MC box coordinates)\n",
    "                global_samples_mc_update.append(sample[6])\n",
    "    \n",
    "    # Convert collected samples into NF coordinates (centered at zero)\n",
    "    global_samples_nf_update = np.array([\n",
    "        np.array([particle - np.array([HALF_BOX, HALF_BOX]) for particle in config])\n",
    "        for config in global_samples_mc_update\n",
    "    ])\n",
    "\n",
    "    # Use cumulative training samples if enabled; otherwise, use just the current cycle's samples\n",
    "    if CUMULATIVE_TRAINING_SAMPLES:\n",
    "        cumulative_training_samples_nf = np.concatenate((cumulative_training_samples_nf, global_samples_nf_update), axis=0)\n",
    "        training_data = cumulative_training_samples_nf\n",
    "    else:\n",
    "        # Simply use the current cycle's samples directly - no need to track old samples\n",
    "        training_data = global_samples_nf_update\n",
    "\n",
    "    # Create a dataloader for the update training set\n",
    "    unique_data = np.unique(training_data, axis=0)\n",
    "    experiment_logger.info(f\"Cycle {cycle+1}: Total unique update samples: {unique_data.shape[0]}\")\n",
    "    dataloader_update = get_dataloader(training_data, NUM_PARTICLES, NUM_DIM, device, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    experiment_logger.info(f\"Cycle {cycle+1}: DataLoader size: {len(dataloader_update.dataset)}, batches: {len(dataloader_update)}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # Training phase: Switch model to training mode and train\n",
    "    # -------------------------------------------------------------------\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize the optimizer for this cycle\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # Single epoch training\n",
    "    cycle_loss = 0.0\n",
    "    for batch in dataloader_update:\n",
    "        optimizer.zero_grad()\n",
    "        # energy_loss,z = model.reverse_kld(BATCH_SIZE) -----> use if you want to test mixed training loss (requires CUDA)\n",
    "        sample_loss = model.forward_kld(batch[0].to(device))\n",
    "        # loss = ALPHA * sample_loss + (1 - ALPHA) * energy_loss -----> use if you want to test mixed training loss \n",
    "        loss = sample_loss\n",
    "        if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        cycle_loss += loss.item()\n",
    "    \n",
    "    avg_cycle_loss = cycle_loss / len(dataloader_update)\n",
    "    cumulative_loss_history.append(avg_cycle_loss)\n",
    "    training_logger.info(f'Cycle {cycle+1}, Loss: {avg_cycle_loss:.4f}')\n",
    "    \n",
    "    # Plot loss every checkpoint interval cycles or on the final cycle\n",
    "    if (cycle + 1) % CHECKPOINT_INTERVAL == 0 or cycle == NUM_TRAINING_CYCLES - 1:\n",
    "        loss_plot_path_svg, loss_plot_path_png = plot_loss(\n",
    "            cumulative_loss_history,\n",
    "            training_directory + \"/\",\n",
    "            base_filename=f\"loss_function\"\n",
    "        )\n",
    "        training_logger.info(f\"Loss plot updated at cycle {cycle+1} and saved to: {loss_plot_path_svg} and {loss_plot_path_png}\")\n",
    "    \n",
    "    # Save the model periodically\n",
    "    if (cycle + 1) % CHECKPOINT_INTERVAL*2 == 0 or cycle == NUM_TRAINING_CYCLES - 1:\n",
    "        model_path = os.path.join(training_directory, f\"model_checkpoint_cycle_{cycle+1}.pth\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        training_logger.info(f\"Model checkpoint saved at cycle {cycle+1} to: {model_path}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # Refeeding Phase: Evaluate model and track acceptance statistics\n",
    "    # -------------------------------------------------------------------\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate samples for MC runs (keeping this for z_ generation consistency)\n",
    "    z = model.sample(NUM_MC_RUNS)\n",
    "    z_ = z.reshape(-1, NUM_PARTICLES, NUM_DIM)\n",
    "    z_ = z_.to('cpu').detach().numpy()\n",
    "    z_ = z_ + HALF_BOX\n",
    "    \n",
    "    # New method: Continue offering configurations until target percentage of MC runs have accepted\n",
    "    if (cycle + 1) % CHECKPOINT_INTERVAL == 0 or cycle == NUM_TRAINING_CYCLES - 1:\n",
    "        production_batch_size = 1000  # Fixed batch size for production\n",
    "        n_iterations = (NUM_SAMPLES_FOR_ANALYSIS + production_batch_size - 1) // production_batch_size  # Ceiling division\n",
    "        all_samples = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(n_iterations):\n",
    "                # Generate samples in smaller batches\n",
    "                a = model.sample(production_batch_size)\n",
    "                # Reshape as required\n",
    "                a_ = a.reshape(-1, NUM_PARTICLES, NUM_DIM)\n",
    "                # Move to CPU and convert to NumPy\n",
    "                a_ = a_.to('cpu').detach().numpy()\n",
    "                # Add HALF_BOX to each element in the samples\n",
    "                a_ = a_ + HALF_BOX\n",
    "                all_samples.append(a_)\n",
    "\n",
    "        a_ = np.concatenate(all_samples, axis=0)\n",
    "        \n",
    "        samples_path = os.path.join(training_directory, \"samples.npy\")\n",
    "        np.save(samples_path, a_)\n",
    "        training_logger.info(f\"Cycle {cycle+1}: Saved {NUM_SAMPLES_FOR_ANALYSIS} samples to {samples_path}\")\n",
    "\n",
    "        # Create heatmap\n",
    "        heatmap_path_svg, heatmap_path_png = plot_frequency_heatmap(\n",
    "            a_ - HALF_BOX,  # Convert back to centered coordinates for plotting\n",
    "            training_directory + \"/\", \n",
    "            cmap_div, \n",
    "            HALF_BOX,\n",
    "            base_filename=f\"frequency_heatmap_cycle_{cycle+1}\"\n",
    "        )\n",
    "        print(f\"Frequency heatmap saved successfully to: {heatmap_path_svg} and {heatmap_path_png}\")\n",
    "\n",
    "        # Calculate and plot pair correlation function\n",
    "        r_vals, g_r = calculate_pair_correlation(a_ - HALF_BOX, NUM_PARTICLES, HALF_BOX, dr=HALF_BOX/50)\n",
    "        pair_corr_path_svg, pair_corr_path_png = plot_pair_correlation(\n",
    "            r_vals, \n",
    "            g_r, \n",
    "            training_directory + \"/\", \n",
    "            base_filename=f\"pair_correlation_function_{cycle+1}\"\n",
    "        )\n",
    "        print(f\"Pair correlation function plot saved successfully to: {pair_corr_path_svg} and {pair_corr_path_png}\")\n",
    "\n",
    "    # Track acceptance statistics\n",
    "    nf_mc_accept_update = 0\n",
    "    nf_to_mc_attempts_update = 0\n",
    "    target_num_accepts = int(NF_TARGET_ACC * NUM_MC_RUNS)\n",
    "    accepted_runs = set()\n",
    "\n",
    "    for mc_run in mc_runs:\n",
    "        mc_run.set_nf_model(model)\n",
    "    \n",
    "    for sample_idx in range(NUM_MC_RUNS):\n",
    "        mc_run = mc_runs[sample_idx]\n",
    "        \n",
    "        # Test the configuration\n",
    "        test_config = z_[sample_idx]\n",
    "        nf_to_mc_attempts_update += 1\n",
    "        accepted = mc_run.nf_big_move(test_config)\n",
    "        \n",
    "        if accepted:\n",
    "            nf_mc_accept_update += 1\n",
    "            accepted_runs.add(sample_idx)\n",
    "            training_logger.info(f\"Cycle {cycle+1}: MC run {sample_idx+1} accepted a configuration\")\n",
    "    \n",
    "    # Calculate and log acceptance statistics\n",
    "    p_acc_update = nf_mc_accept_update / nf_to_mc_attempts_update if nf_to_mc_attempts_update > 0 else 0\n",
    "    p_acc_history.append(p_acc_update)\n",
    "    mcmc_steps_history.append(total_mcmc_steps)\n",
    "    \n",
    "    # Update global counters\n",
    "    big_move_attempts += nf_to_mc_attempts_update\n",
    "    big_move_accepts += nf_mc_accept_update\n",
    "    \n",
    "    training_logger.info(f\"Cycle {cycle+1}: {len(accepted_runs)}/{NUM_MC_RUNS} MC runs accepted a configuration\")\n",
    "    training_logger.info(f\"Cycle {cycle+1}: Tested {nf_to_mc_attempts_update} configurations total\")\n",
    "    training_logger.info(f\"Cycle {cycle+1}: Overall acceptance rate: {p_acc_update:.4f}\")\n",
    "    experiment_logger.info(f\"Cycle {cycle+1}: Target reached after testing {nf_to_mc_attempts_update} configurations with p_accept = {p_acc_update:.4f}\")\n",
    "    print(f\"Acceptance stats: {len(accepted_runs)}/{NUM_MC_RUNS} MC runs accepted. p(accept) = {p_acc_update:.4f} after {nf_to_mc_attempts_update} attempts\")\n",
    "    \n",
    "    # Plot acceptance rate every checkpoint cycles or on the final cycle\n",
    "    if (cycle + 1) % CHECKPOINT_INTERVAL == 0 or cycle == NUM_TRAINING_CYCLES - 1:\n",
    "        acc_plot_path_svg, acc_plot_path_png = plot_acceptance_rate(\n",
    "            p_acc_history,\n",
    "            training_directory + \"/\",\n",
    "            x_values=mcmc_steps_history,\n",
    "            xlabel='MCMC Steps',\n",
    "            base_filename=\"nf_acceptance_rate_mcmc_steps\"\n",
    "        )\n",
    "        training_logger.info(f\"Acceptance rate vs MCMC steps plot updated at cycle {cycle+1} and saved to: {acc_plot_path_svg} and {acc_plot_path_png}\")\n",
    "\n",
    "    experiment_logger.info(f\"Finished update training cycle {cycle+1}/{NUM_TRAINING_CYCLES}\")\n",
    "\n",
    "final_model_samples = a_ \n",
    "\n",
    "# -------------------------\n",
    "# After all update cycles: Plot p_acc_history as a function of total training samples\n",
    "# -------------------------\n",
    "\n",
    "# Compute the cumulative number of training samples used at each point.\n",
    "# The initial model was trained on INITIAL_TRAINING_NUM_SAMPLES samples.\n",
    "# Then, each update cycle adds UPDATE_NUM_SAMPLES samples.\n",
    "total_samples_trained = [0, INITIAL_TRAINING_NUM_SAMPLES]\n",
    "for i in range(1, NUM_TRAINING_CYCLES + 1):\n",
    "    total_samples_trained.append(total_samples_trained[-1] + UPDATE_NUM_SAMPLES)\n",
    "\n",
    "# Save the data used for plotting in a JSON file for future reference\n",
    "plot_data = {\n",
    "    \"total_samples_trained\": total_samples_trained,\n",
    "    \"p_acc_history\": p_acc_history\n",
    "}\n",
    "data_save_path = os.path.join(directory, \"p_acc_history_data.json\")\n",
    "with open(data_save_path, \"w\") as f:\n",
    "    json.dump(plot_data, f, indent=4)\n",
    "experiment_logger.info(f\"Plot data saved successfully to: {data_save_path}\")\n",
    "\n",
    "# Plot the acceptance history with utility function\n",
    "acc_vs_samples_path_svg, acc_vs_samples_path_png = plot_acceptance_rate(\n",
    "    p_acc_history,\n",
    "    directory + \"/\",\n",
    "    x_values=total_samples_trained,\n",
    "    xlabel='Training Samples'\n",
    ")\n",
    "experiment_logger.info(f\"Acceptance history plot saved successfully to: {acc_vs_samples_path_svg} and {acc_vs_samples_path_png}\")\n",
    "\n",
    "# -------------------------\n",
    "# After the full experiment, running the mc_runs with the samples from the final trained model\n",
    "# -------------------------\n",
    "trained_nf_model = model  # this is your trained normalizing flow model\n",
    "# Initialize free_energy_array outside the loop to collect data from all runs\n",
    "free_energy_array = []\n",
    "\n",
    "# After the testing phase, plot average X coordinate vs. step number using the stored testing configurations.\n",
    "for run_idx, mc_run in enumerate(mc_runs):\n",
    "    if mc_run.testing_samples:\n",
    "        testing_configs = np.array(mc_run.testing_samples)  # shape: (num_samples, num_particles, 2)\n",
    "        \n",
    "        # Compute well statistics\n",
    "        avg_x_values, p_a_values, p_b_values, deltaF_normalized_values, runs = calculate_well_statistics(\n",
    "            testing_configs, 0, HALF_BOX, R0\n",
    "        )\n",
    "\n",
    "        # Add this run's free energy data to the array\n",
    "        free_energy_array.append(deltaF_normalized_values)\n",
    "\n",
    "        # Plot well statistics\n",
    "        run_folder = os.path.join(mc_runs_directory, f\"run_{run_idx+1:03d}\")\n",
    "        well_stats_path_svg, well_stats_path_png = plot_well_statistics(\n",
    "            avg_x_values, \n",
    "            p_a_values, \n",
    "            p_b_values, \n",
    "            deltaF_normalized_values, \n",
    "            runs, \n",
    "            HALF_BOX,\n",
    "            run_folder\n",
    "        )\n",
    "        \n",
    "        # Plot average x coordinate for individual particles\n",
    "        avg_x_path_svg, avg_x_path_png = plot_avg_x_coordinate(\n",
    "            testing_configs,\n",
    "            run_folder,\n",
    "            HALF_BOX,\n",
    "            run_idx+1\n",
    "        )\n",
    "\n",
    "# Plot summary of the first 10 MC runs if there are at least 10\n",
    "if len(mc_runs) >= 10:\n",
    "    multi_avg_x_path_svg, multi_avg_x_path_png = plot_multiple_avg_x_coordinates(\n",
    "        mc_runs,\n",
    "        directory\n",
    "    )\n",
    "    experiment_logger.info(f\"Summary plot of first 10 MC runs saved to: {multi_avg_x_path_svg} and {multi_avg_x_path_png}\")\n",
    "\n",
    "# Plot average free energy across all runs\n",
    "free_energy_plot_path_svg, free_energy_plot_path_png, final_mean, final_sem, final_std = plot_avg_free_energy(\n",
    "    free_energy_array,\n",
    "    directory,\n",
    "    color='C5'\n",
    ")\n",
    "experiment_logger.info(f\"Average free energy plot saved to: {free_energy_plot_path_svg} and {free_energy_plot_path_png}\")\n",
    "experiment_logger.info(f\"Final mean delta F = {final_mean}\")\n",
    "experiment_logger.info(f\"Final standard error delta F = {final_sem}\")\n",
    "experiment_logger.info(f\"Final std delta F = {final_std}\")\n",
    "\n",
    "# -------------------------\n",
    "# After the full experiment, save each MC run's sampled data to a CSV file and the configurations (sample[6] for each sample) are saved as an NPY file\n",
    "# -------------------------\n",
    "for i, mc_run in enumerate(mc_runs):\n",
    "    run_folder = os.path.join(mc_runs_directory, f\"run_{i+1:03d}\")\n",
    "    csv_filename = os.path.join(run_folder, 'sampled_data.csv')\n",
    "    print(f\"Saving sampled data to {csv_filename}\")\n",
    "    \n",
    "    try:\n",
    "        with open(csv_filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\n",
    "                \"cycle_number\",\n",
    "                \"energy_per_particle\",\n",
    "                \"density\",\n",
    "                \"pressure\",\n",
    "                \"box_size_x\",\n",
    "                \"box_size_y\",\n",
    "                \"particle_configuration\"\n",
    "            ])\n",
    "            \n",
    "            for data in mc_run.local_samples:\n",
    "                # Expecting that each sample is structured as:\n",
    "                # (cycle_number, energy_per_particle, density, pressure, box_size_x, box_size_y, particles)\n",
    "                cycle_number, energy_per_particle, density, pressure, box_size_x, box_size_y, particles = data\n",
    "                \n",
    "                # Convert the particle configuration to a NumPy array and flatten it.\n",
    "                particles_flat = np.array(particles).flatten()\n",
    "                \n",
    "                writer.writerow([\n",
    "                    cycle_number,\n",
    "                    energy_per_particle,\n",
    "                    density,\n",
    "                    pressure,\n",
    "                    box_size_x,\n",
    "                    box_size_y,\n",
    "                    particles_flat.tolist()  # storing as a JSON-like list in the CSV\n",
    "                ])\n",
    "        experiment_logger.info(f\"MC run {i+1} sampled data successfully saved to {csv_filename}\")\n",
    "    except Exception as e:\n",
    "        experiment_logger.error(f\"Error: Failed to save sampled data for MC run {i+1} to {csv_filename}. Error: {e}\")\n",
    "\n",
    "    # Extract and save local samples configurations\n",
    "    configs = np.array([sample[6] for sample in mc_run.local_samples])\n",
    "    configs_filepath = os.path.join(run_folder, \"mc_run_configs.npy\")\n",
    "    np.save(configs_filepath, configs)\n",
    "    experiment_logger.info(f\"MC run {i+1} local config data saved to: {configs_filepath}\")\n",
    "\n",
    "    # Extract and save testing samples configurations\n",
    "    testing_configs = np.array(mc_run.testing_samples)\n",
    "    testing_configs_filepath = os.path.join(run_folder, \"mc_run_testing_configs.npy\") \n",
    "    np.save(testing_configs_filepath, testing_configs)\n",
    "    experiment_logger.info(f\"MC run {i+1} testing config data saved to: {testing_configs_filepath}\")\n",
    "\n",
    "# After all update cycles, plot the acceptance rate history\n",
    "acc_plot_path_svg, acc_plot_path_png = plot_acceptance_rate(\n",
    "    p_acc_history,\n",
    "    directory + \"/\",\n",
    "    x_values=mcmc_steps_history,\n",
    "    xlabel='MCMC Steps',\n",
    "    base_filename=\"nf_acceptance_rate_mcmc_steps\",\n",
    "    color='C5'\n",
    ")\n",
    "experiment_logger.info(f\"Acceptance rate vs MCMC steps plot saved to: {acc_plot_path_svg} and {acc_plot_path_png}\")\n",
    "\n",
    "# Save the acceptance rate data to a CSV file\n",
    "acceptance_data_path = os.path.join(directory, \"acceptance_rate_mcmc_steps_data.csv\")\n",
    "with open(acceptance_data_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['MCMC_Steps', 'Acceptance_Rate'])\n",
    "    for steps, acc_rate in zip(mcmc_steps_history, p_acc_history):\n",
    "        writer.writerow([steps, acc_rate])\n",
    "experiment_logger.info(f\"Acceptance rate vs MCMC steps data saved to: {acceptance_data_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repo_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
