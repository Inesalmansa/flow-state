{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid NF-MCMC Demo - Algorithm 1: Pre-trainging NF\n",
    "### Run script to run example Algorithm 1 experiment for dV = 0\n",
    "\n",
    "Total script run time ~30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from scipy.spatial.distance import squareform, cdist\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from demo_utils import get_project_root\n",
    "# Add the custom path for hybrid_nf_mcmc\n",
    "project_root = get_project_root()\n",
    "hybrid_nf_mcmc_path = os.path.join(project_root, \"hybrid_NF_MCMC\")\n",
    "sys.path.append(hybrid_nf_mcmc_path)\n",
    "from utils import (get_project_root, setup_logger, get_dataloader, calculate_well_statistics, \n",
    "                  set_icl_color_cycle, get_icl_heatmap_cmap, plot_loss, \n",
    "                  plot_frequency_heatmap, generate_samples, \n",
    "                  calculate_pair_correlation, plot_pair_correlation, save_rdf_data, plot_acceptance_rate,\n",
    "                  plot_avg_free_energy, plot_well_statistics, plot_avg_x_coordinate,\n",
    "                  plot_multiple_avg_x_coordinates)\n",
    "import argparse\n",
    "import csv\n",
    "\n",
    "set_icl_color_cycle()\n",
    "cmap_div = get_icl_heatmap_cmap(\"diverging\")\n",
    "\n",
    "# importing the NF and MC modules\n",
    "project_root = get_project_root()\n",
    "nf_path = os.path.join(project_root, \"NF\")\n",
    "sys.path.append(nf_path)\n",
    "sys.path.append(project_root)\n",
    "import normflows as NF\n",
    "import MCMC as MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter inputs\n",
    "\n",
    "To increase accuracy and precision run for the number of training cycles and with number of parallel MCMC runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"A1_demo\"\n",
    "\n",
    "# EXPERIMENT VARIABLES\n",
    "NUM_MC_RUNS = 10       # how many mc run running in 'parallel'\n",
    "MASTER_SEED = 42        # master seed to give seeds for each mc run to run differently\n",
    "NUM_PARTICLES = 3\n",
    "NUM_DIM = 2\n",
    "NUM_TRAINING_CYCLES = 0\n",
    "\n",
    "# mc needed params\n",
    "TEMP = 1.0\n",
    "RHO = 0.03\n",
    "ASPECT_RATIO = 1.0\n",
    "VISUALISE = True\n",
    "CHECKING = True\n",
    "EQUILIBRATION_STEPS = 5000\n",
    "NUM_WELLS = 2\n",
    "V0_LIST = [-10.0,-10.0]              # depth of wells - adjust to change the dV\n",
    "R0 = 1.2                \n",
    "K_VAL = 15             \n",
    "HALF_BOX = ((NUM_PARTICLES/RHO)**(1/NUM_DIM))/2\n",
    "INITIAL_MAX_DISPLACEMENT = 0.65\n",
    "SAMPLING_FREQUENCY = 150\n",
    "ADJUSTING_FREQUENCY = 5000\n",
    "\n",
    "# nf needed params\n",
    "INITIAL_TRAINING_NUM_SAMPLES = 10240\n",
    "BATCH_SIZE = 512\n",
    "K = 15\n",
    "EPOCHS = 20\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 0\n",
    "UPDATE_NUM_SAMPLES = 0\n",
    "HIDDEN_UNITS = 256\n",
    "NUM_BINS = 32\n",
    "N_BLOCKS = 8\n",
    "\n",
    "# post training runs parameters\n",
    "TESTING_SAMPLING_FREQUENCY = 10\n",
    "TESTING = True\n",
    "BIG_MOVE_ATTEMPTS = 20\n",
    "BIG_MOVE_INTERVAL = 100\n",
    "NUM_SAMPLES_FOR_ANALYSIS = BIG_MOVE_ATTEMPTS * NUM_MC_RUNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving parameters to a JSON file, setting up directory and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(project_root, \"demos/data\", experiment_name)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "# Setup a global experiment logger that logs to a file in the experiment directory.\n",
    "experiment_log_file = os.path.join(directory, \"experiment.log\")\n",
    "experiment_logger = setup_logger(\"experiment\", experiment_log_file, file_level=logging.DEBUG, stream_level=logging.INFO)\n",
    "experiment_logger.info(\"half box is: \" + str(HALF_BOX))\n",
    "experiment_logger.info(f\"Directory created at: {directory}\")\n",
    "\n",
    "mc_runs_directory = os.path.join(directory, \"mc_runs\")\n",
    "os.makedirs(mc_runs_directory, exist_ok=True)\n",
    "training_rounds_directory = os.path.join(directory, \"training\")\n",
    "os.makedirs(training_rounds_directory, exist_ok=True)\n",
    "\n",
    "# Save experiment parameters to a JSON file in the parent experiment directory\n",
    "experiment_params = {\n",
    "    \"NUM_MC_RUNS\": NUM_MC_RUNS,\n",
    "    \"MASTER_SEED\": MASTER_SEED,\n",
    "    \"NUM_PARTICLES\": NUM_PARTICLES,\n",
    "    \"NUM_DIM\": NUM_DIM,\n",
    "    \"NUM_TRAINING_CYCLES\": NUM_TRAINING_CYCLES,\n",
    "    \"INITIAL_TRAINING_NUM_SAMPLES\": INITIAL_TRAINING_NUM_SAMPLES,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"K\": K,\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"LR\": LR,\n",
    "    \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
    "    \"UPDATE_NUM_SAMPLES\": UPDATE_NUM_SAMPLES,\n",
    "    \"HIDDEN_UNITS\": HIDDEN_UNITS,\n",
    "    \"NUM_BINS\": NUM_BINS,\n",
    "    \"N_BLOCKS\": N_BLOCKS,\n",
    "    \"TEMP\": TEMP,\n",
    "    \"RHO\": RHO,\n",
    "    \"ASPECT_RATIO\": ASPECT_RATIO,\n",
    "    \"VISUALISE\": VISUALISE,\n",
    "    \"CHECKING\": CHECKING,\n",
    "    \"EQUILIBRATION_STEPS\": EQUILIBRATION_STEPS,\n",
    "    \"NUM_WELLS\": NUM_WELLS,\n",
    "    \"V0_LIST\": V0_LIST,\n",
    "    \"R0\": R0,\n",
    "    \"K_VAL\": K_VAL,\n",
    "    \"HALF_BOX\": HALF_BOX,\n",
    "    \"INITIAL_MAX_DISPLACEMENT\": INITIAL_MAX_DISPLACEMENT,\n",
    "    \"SAMPLING_FREQUENCY\": SAMPLING_FREQUENCY,\n",
    "    \"ADJUSTING_FREQUENCY\": ADJUSTING_FREQUENCY,\n",
    "    \"TESTING\": TESTING,\n",
    "    \"TESTING_SAMPLING_FREQUENCY\": TESTING_SAMPLING_FREQUENCY,\n",
    "    \"BIG_MOVE_ATTEMPTS\": BIG_MOVE_ATTEMPTS,\n",
    "    \"BIG_MOVE_INTERVAL\": BIG_MOVE_INTERVAL,\n",
    "    \"NUM_SAMPLES_FOR_ANALYSIS\": NUM_SAMPLES_FOR_ANALYSIS\n",
    "}\n",
    "json_file_path = os.path.join(directory, \"params.json\")\n",
    "with open(json_file_path, \"w\") as f:\n",
    "    json.dump(experiment_params, f, indent=4)\n",
    "experiment_logger.info(f\"Experiment parameters saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation and data collection: initialise MCMC runs, equilibrate them and collect samples for NF pre-training\n",
    "\n",
    "Run-time: ~5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation of experiment - sets up the monte carlo runs and equilibrates and collects samples\n",
    "mc_runs = []  # list to store each Monte Carlo simulation instance\n",
    "for i in range(NUM_MC_RUNS):\n",
    "    seed = i + MASTER_SEED\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create a dedicated folder and logger for this Monte Carlo run.\n",
    "    run_dir = os.path.join(mc_runs_directory, f\"run_{i+1:03d}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    mc_log_file = os.path.join(run_dir, \"mc_run.log\")\n",
    "    mc_logger = setup_logger(f\"MC_run_{i+1:03d}\", mc_log_file, file_level=logging.DEBUG, stream_level=logging.WARNING)\n",
    "\n",
    "    # even i runs initialise left and odd initialise right\n",
    "    if i % 2 == 0:\n",
    "        particles, sim_box = MC.initialise_low_left(\n",
    "                num_particles=NUM_PARTICLES,\n",
    "                rho=RHO,\n",
    "                aspect_ratio=ASPECT_RATIO,\n",
    "                visualise=False,\n",
    "                checking=False\n",
    "            )\n",
    "        experiment_logger.info(f\"run {i} starts in left\")\n",
    "    else:\n",
    "        particles, sim_box = MC.initialise_low_right(\n",
    "                num_particles=NUM_PARTICLES,\n",
    "                rho=RHO,\n",
    "                aspect_ratio=ASPECT_RATIO,\n",
    "                visualise=False,\n",
    "                checking=False\n",
    "            )\n",
    "        experiment_logger.info(f\"run {i} starts in right\")\n",
    "        \n",
    "    mc_run = MC.MonteCarlo(\n",
    "        particles=particles,\n",
    "        sim_box=sim_box,\n",
    "        temperature=TEMP,\n",
    "        num_particles=NUM_PARTICLES,\n",
    "        num_wells=NUM_WELLS,    # Pass number of wells\n",
    "        V0_list=V0_LIST,                  # Pass well depth\n",
    "        r0=R0,\n",
    "        k=K_VAL,\n",
    "        initial_max_displacement=INITIAL_MAX_DISPLACEMENT,  # Adjust as needed\n",
    "        target_acceptance=0.5,\n",
    "        timing=False,\n",
    "        checking=CHECKING,\n",
    "        logger=mc_logger,       # Each run now logs to its own file\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    mc_runs.append(mc_run)\n",
    "    experiment_logger.info(f\"Monte Carlo run {i} initialised and stored\")\n",
    "\n",
    "# Plot the potential wells for visualization\n",
    "MC.visualise.plot_potential(\n",
    "    box_siz_x=sim_box.box_size_x,\n",
    "    box_size_y=sim_box.box_size_y,\n",
    "    V0_list=V0_LIST,\n",
    "    r0=R0,\n",
    "    k=K_VAL,\n",
    "    num_wells=NUM_WELLS,\n",
    "    output_path=directory\n",
    ")\n",
    "\n",
    "experiment_logger.info(\"All Monte Carlo runs initialised\")\n",
    "experiment_logger.info(\"list of run instances: \" + str(mc_runs))\n",
    "\n",
    "# equilibration of each mc run\n",
    "for mc_run in mc_runs:\n",
    "    for step in range(1, EQUILIBRATION_STEPS + 1):\n",
    "        mc_run.particle_displacement()\n",
    "        if step % ADJUSTING_FREQUENCY == 0:\n",
    "            mc_run.adjust_displacement()\n",
    "        if step % SAMPLING_FREQUENCY == 0:\n",
    "            sample = mc_run.sample(step)\n",
    "            mc_run.local_samples.append(sample)\n",
    "    \n",
    "# Plot and save the equilibrated configuration for each MC run\n",
    "for i, mc_run in enumerate(mc_runs):\n",
    "    run_dir = os.path.join(mc_runs_directory, f\"run_{i+1:03d}\")\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    current_config = mc_run.particles\n",
    "    plt.scatter(current_config[:, 0], current_config[:, 1], alpha=0.6)\n",
    "    plt.xlim(0, 2*HALF_BOX)\n",
    "    plt.ylim(0, 2*HALF_BOX)\n",
    "    plt.xlabel(r'$x$')\n",
    "    plt.ylabel(r'$y$') \n",
    "    plt.title(f'Equilibrated Configuration - MC Run {i+1}')\n",
    "    \n",
    "    # Save plot to the MC run's directory\n",
    "    config_plot_path = os.path.join(run_dir, f\"equilibrated_config.png\")\n",
    "    fig.savefig(config_plot_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    mc_run.logger.info(f\"Equilibrated configuration plot saved to: {config_plot_path}\")\n",
    "\n",
    "# Initialize MCMC step counter right after equilibration\n",
    "total_mcmc_steps = 0\n",
    "\n",
    "# Initialize acceptance rate tracking at the very beginning\n",
    "p_acc_history = [0.0]  # Initial acceptance rate is 0\n",
    "mcmc_steps_history = [0]  # Start from MCMC step 0\n",
    "big_move_attempts = 0\n",
    "big_move_accepts = 0\n",
    "\n",
    "\n",
    "# collect initial set of training samples\n",
    "global_samples_mc = []  # storage of mc samples in mc box range of 0,10\n",
    "production_runs = int(INITIAL_TRAINING_NUM_SAMPLES/(NUM_MC_RUNS / SAMPLING_FREQUENCY))\n",
    "experiment_logger.info(\"production runs per cycle: \" + str(production_runs))\n",
    "for mc_run in mc_runs:\n",
    "    for step in range(1, production_runs + 1):\n",
    "        mc_run.particle_displacement()\n",
    "        # total_mcmc_steps += 1  # Count each MCMC step\n",
    "        if step % SAMPLING_FREQUENCY == 0:\n",
    "            sample = mc_run.sample(step)\n",
    "            mc_run.local_samples.append(sample)\n",
    "            global_samples_mc.append(sample[6])  # only config being stored (in MC box bound)\n",
    "\n",
    "global_samples_nf = np.array([np.array([particle - np.array([HALF_BOX, HALF_BOX]) for particle in config]) for config in global_samples_mc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NF pre-training\n",
    "\n",
    "Run-time: ~25 minutes on a M1 chip (no CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepping initial samples as for dataloader\n",
    "enable_cuda = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n",
    "experiment_logger.info('device is: ' + str(device))\n",
    "\n",
    "unique_data = np.unique(global_samples_nf, axis=0)\n",
    "experiment_logger.info(\"Total unique samples: \" + str(unique_data.shape[0]))\n",
    "\n",
    "dataloader = get_dataloader(global_samples_nf, NUM_PARTICLES, NUM_DIM, device, batch_size=BATCH_SIZE, shuffle=True)\n",
    "experiment_logger.info(\"DataLoader details:\")\n",
    "experiment_logger.info(\"Dataset size: \" + str(len(dataloader.dataset)))\n",
    "experiment_logger.info(\"Number of batches: \" + str(len(dataloader)))\n",
    "first_batch = next(iter(dataloader))\n",
    "experiment_logger.info(\"Shape of data in first batch: \" + str(first_batch[0].shape))\n",
    "\n",
    "# initialise the normalizing flow model\n",
    "bound = HALF_BOX\n",
    "base = NF.Energy.UniformParticle(NUM_PARTICLES, NUM_DIM, bound, device=device)\n",
    "# target = NF.Energy.SimpleLJ(NUM_PARTICLES * NUM_DIM, NUM_PARTICLES, 1, bound)    # not used for now\n",
    "# K = 15 - now defined in the argument of function\n",
    "flow_layers = []\n",
    "for i in range(K):\n",
    "    flow_layers += [NF.flows.CircularCoupledRationalQuadraticSpline(NUM_PARTICLES * NUM_DIM, NUM_BINS, HIDDEN_UNITS,\n",
    "                    range(NUM_PARTICLES * NUM_DIM), num_bins=NUM_BINS, tail_bound=bound)]\n",
    "model = NF.NormalizingFlow(base, flow_layers).to(device)\n",
    "experiment_logger.info(f'Model prepared with {NUM_PARTICLES} particles and {NUM_DIM} dimensions!')\n",
    "\n",
    "# Create a dedicated folder and logger for the first training round of the normalizing flow.\n",
    "nf_training_dir = os.path.join(training_rounds_directory, \"pre_training_results\")\n",
    "os.makedirs(nf_training_dir, exist_ok=True)\n",
    "nf_log_file = os.path.join(nf_training_dir, \"nf_training_round.log\")\n",
    "nf_logger = setup_logger(\"NF_training_round\", nf_log_file, file_level=logging.DEBUG, stream_level=logging.WARNING)\n",
    "\n",
    "# train the normalizing flow on initial training batch\n",
    "loss_hist = np.array([])\n",
    "loss_epoch = []\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "experiment_logger.info(f'Optimizer prepared with learning rate: {LR}')\n",
    "data_save = []\n",
    "\n",
    "for it in trange(EPOCHS, desc=\"Training Progress\"):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # Compute loss\n",
    "        # loss,z = model.reverse_kld(num_samples)\n",
    "        loss = model.forward_kld(batch[0].to(device))\n",
    "        # Do backprop and optimizer step\n",
    "        # if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            experiment_logger.warning(\"Warning: Loss is NaN or Inf. Skipping this batch.\")\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "        # Log loss\n",
    "        loss_hist = np.append(loss_hist, loss.to('cpu').data.numpy())\n",
    "        epoch_loss += loss.item()\n",
    "    loss_epoch.append(epoch_loss / len(dataloader))\n",
    "    nf_logger.info(f'Epoch {it+1}/{EPOCHS}, Loss: {loss_epoch[-1]:.4f}')\n",
    "\n",
    "# Plot loss\n",
    "loss_plot_path_svg, loss_plot_path_png = plot_loss(loss_epoch, nf_training_dir)\n",
    "nf_logger.info(f\"Loss plot saved successfully to: {loss_plot_path_png}\")\n",
    "\n",
    "model_path = os.path.join(nf_training_dir, \"pre_trained_model_circular_spline_res_dense.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "nf_logger.info(f\"Model saved successfully to: {model_path}\")\n",
    "\n",
    "# change to eval mode to test the NF model\n",
    "model.eval()\n",
    "samples_for_mc = []\n",
    "z = model.sample(NUM_MC_RUNS)\n",
    "z_ = z.reshape(-1, NUM_PARTICLES, NUM_DIM)  # Reshape as required\n",
    "z_ = z_.to('cpu').detach().numpy()  # Move to CPU and convert to NumPy\n",
    "z_ = z_ + HALF_BOX  # Add HALF_BOX to each element in the samples\n",
    "print(z_)\n",
    "\n",
    "# Generate samples\n",
    "a_ = generate_samples(model, NUM_PARTICLES, NUM_DIM, \n",
    "                     n_iterations=NUM_SAMPLES_FOR_ANALYSIS // 5000 + 1, \n",
    "                     samples_per_iteration=5000)\n",
    "a_ = a_ + HALF_BOX  # Add HALF_BOX to each element in the samples\n",
    "print(a_.shape)\n",
    "samples_path = os.path.join(nf_training_dir, \"samples.npy\")\n",
    "np.save(samples_path, a_)\n",
    "final_model_samples = a_\n",
    "\n",
    "# make heatmap\n",
    "heatmap_path_svg, heatmap_path_png = plot_frequency_heatmap(a_- HALF_BOX, nf_training_dir, cmap_div, HALF_BOX)\n",
    "nf_logger.info(f\"Frequency heatmap saved successfully to: {heatmap_path_png}\")\n",
    "\n",
    "# Calculate and plot pair correlation function\n",
    "r_vals, g_r = calculate_pair_correlation(a_ - HALF_BOX, NUM_PARTICLES, HALF_BOX, dr=HALF_BOX/50)\n",
    "pair_corr_path_svg, pair_corr_path_png = plot_pair_correlation(\n",
    "    r_vals, \n",
    "    g_r, \n",
    "    nf_training_dir\n",
    ")\n",
    "print(f\"Pair correlation function plot saved successfully to: {pair_corr_path_svg} and {pair_corr_path_png}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post training hybrid NF-MCMC testing phase\n",
    "\n",
    "Run-time: ~1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# After the full training, running the mc_runs with the samples from the final trained model\n",
    "# -------------------------\n",
    "trained_nf_model = model \n",
    "\n",
    "# For each Monte Carlo simulation instance, assign the trained model:\n",
    "for mc_run in mc_runs:\n",
    "    mc_run.set_nf_model(trained_nf_model)\n",
    "\n",
    "if TESTING:\n",
    "    test_configs = final_model_samples  # configurations produced by the final model for big move suggestions\n",
    "    \n",
    "    free_energy_array = []\n",
    "    \n",
    "    # Initialize counters for acceptance statistics\n",
    "    nf_to_mc_attempts_total = 0\n",
    "    nf_mc_accept_total = 0\n",
    "    accepted_runs = set()  # Track which runs accepted the move in each cycle\n",
    "\n",
    "    # Testing phase: process all MC runs in parallel for BIG_MOVE_ATTEMPTS cycles\n",
    "    for cycle in range(BIG_MOVE_ATTEMPTS):\n",
    "        # First, run displacement steps for all MC runs\n",
    "        for run_idx, mc_run in enumerate(mc_runs):\n",
    "            for step in range(1, BIG_MOVE_INTERVAL + 1):\n",
    "                mc_run.particle_displacement()\n",
    "                total_mcmc_steps += 1\n",
    "                if total_mcmc_steps % TESTING_SAMPLING_FREQUENCY == 0:\n",
    "                    sample = mc_run.sample(step)\n",
    "                    mc_run.local_samples.append(sample)\n",
    "                    mc_run.testing_samples.append(sample[6])  # store configuration (in MC box coordinates)\n",
    "        \n",
    "        # Reset accepted runs for this cycle\n",
    "        accepted_runs.clear()\n",
    "        \n",
    "        # Make big move attempts for all MC runs\n",
    "        for run_idx, mc_run in enumerate(mc_runs):\n",
    "            # Each MC run receives a unique configuration for a big move suggestion\n",
    "            test_config = test_configs[cycle * len(mc_runs) + run_idx]\n",
    "            \n",
    "            # Attempt the big move\n",
    "            nf_to_mc_attempts_total += 1\n",
    "            accepted = mc_run.nf_big_move(test_config)\n",
    "            \n",
    "            if accepted:\n",
    "                nf_mc_accept_total += 1\n",
    "                accepted_runs.add(run_idx)\n",
    "                \n",
    "                # Initialize testing move counters if they don't exist yet\n",
    "                if not hasattr(mc_run, 'test_moves_accepted'):\n",
    "                    mc_run.test_moves_attempted = 0\n",
    "                    mc_run.test_moves_accepted = 0\n",
    "                \n",
    "                # Update testing counters for this simulation run\n",
    "                mc_run.test_moves_attempted = mc_run.test_moves_attempted + 1 if hasattr(mc_run, 'test_moves_attempted') else 1\n",
    "                mc_run.test_moves_accepted += 1\n",
    "                \n",
    "        \n",
    "        # Calculate and log acceptance statistics for this cycle\n",
    "        p_acc_current = nf_mc_accept_total / nf_to_mc_attempts_total if nf_to_mc_attempts_total > 0 else 0\n",
    "        p_acc_history.append(p_acc_current)\n",
    "        mcmc_steps_history.append(total_mcmc_steps)\n",
    "        \n",
    "        # Update global counters\n",
    "        big_move_attempts = nf_to_mc_attempts_total\n",
    "        big_move_accepts = nf_mc_accept_total\n",
    "        \n",
    "        experiment_logger.info(f\"Cycle {cycle+1} complete: p_accept = {p_acc_current:.4f} ({nf_mc_accept_total}/{nf_to_mc_attempts_total})\")\n",
    "\n",
    "    # Plot the acceptance rate history\n",
    "    acc_plot_path_svg, acc_plot_path_png = plot_acceptance_rate(\n",
    "        p_acc_history, \n",
    "        directory, \n",
    "        x_values=mcmc_steps_history, \n",
    "        xlabel='MCMC Steps',\n",
    "        base_filename=\"nf_acceptance_rate\",\n",
    "        color='C2'\n",
    "    )\n",
    "    experiment_logger.info(f\"Acceptance rate plot saved to: {acc_plot_path_png}\")\n",
    "    \n",
    "for run_idx, mc_run in enumerate(mc_runs):\n",
    "    if mc_run.testing_samples:\n",
    "        testing_configs = np.array(mc_run.testing_samples)  # shape: (num_samples, num_particles, 2)\n",
    "        testing_configs = testing_configs[:]\n",
    "        \n",
    "        # Compute well statistics\n",
    "        avg_x_values, p_a_values, p_b_values, deltaF_normalized_values, runs = calculate_well_statistics(\n",
    "            testing_configs, 0, HALF_BOX, R0\n",
    "        )\n",
    "\n",
    "        # Add this run's free energy data to the array\n",
    "        free_energy_array.append(deltaF_normalized_values)\n",
    "\n",
    "        # Plot well statistics\n",
    "        run_folder = os.path.join(mc_runs_directory, f\"run_{run_idx+1:03d}\")\n",
    "        well_stats_path_svg, well_stats_path_png = plot_well_statistics(\n",
    "            avg_x_values, \n",
    "            p_a_values, \n",
    "            p_b_values, \n",
    "            deltaF_normalized_values, \n",
    "            runs, \n",
    "            HALF_BOX,\n",
    "            run_folder\n",
    "        )\n",
    "        \n",
    "        # Plot average x coordinate for individual particles\n",
    "        avg_x_path_svg, avg_x_path_png = plot_avg_x_coordinate(\n",
    "            testing_configs,\n",
    "            run_folder,\n",
    "            HALF_BOX,\n",
    "            run_idx+1\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# Plot summary of the first 10 MC runs if there are at least 10\n",
    "if len(mc_runs) >= 10:\n",
    "    multi_avg_x_path_svg, multi_avg_x_path_png = plot_multiple_avg_x_coordinates(\n",
    "        mc_runs,\n",
    "        directory\n",
    "    )\n",
    "    experiment_logger.info(f\"Summary plot of first 10 MC runs saved to: {multi_avg_x_path_svg} and {multi_avg_x_path_png}\")\n",
    "\n",
    "# Plot average free energy across all runs\n",
    "free_energy_plot_path_svg, free_energy_plot_path_png, final_mean, final_sem, final_std = plot_avg_free_energy(\n",
    "    free_energy_array,\n",
    "    directory,\n",
    "    color='C2'\n",
    ")\n",
    "experiment_logger.info(f\"Average free energy plot saved to: {free_energy_plot_path_svg} and {free_energy_plot_path_png}\")\n",
    "experiment_logger.info(f\"Final mean delta F = {final_mean}\")\n",
    "experiment_logger.info(f\"Final standard error delta F = {final_sem}\")\n",
    "experiment_logger.info(f\"Final std delta F = {final_std}\")\n",
    "\n",
    "# -------------------------\n",
    "# After the full experiment, save each MC run's sampled data to a CSV file and the configurations (sample[6] for each sample) are saved as an NPY file\n",
    "# -------------------------\n",
    "for i, mc_run in enumerate(mc_runs):\n",
    "    run_folder = os.path.join(mc_runs_directory, f\"run_{i+1:03d}\")\n",
    "    csv_filename = os.path.join(run_folder, 'sampled_data.csv')\n",
    "    print(f\"Saving sampled data to {csv_filename}\")\n",
    "    \n",
    "    try:\n",
    "        with open(csv_filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\n",
    "                \"cycle_number\",\n",
    "                \"energy_per_particle\",\n",
    "                \"density\",\n",
    "                \"pressure\",\n",
    "                \"box_size_x\",\n",
    "                \"box_size_y\",\n",
    "                \"particle_configuration\"\n",
    "            ])\n",
    "            \n",
    "            for data in mc_run.local_samples:\n",
    "                # Expecting that each sample is structured as:\n",
    "                # (cycle_number, energy_per_particle, density, pressure, box_size_x, box_size_y, particles)\n",
    "                cycle_number, energy_per_particle, density, pressure, box_size_x, box_size_y, particles = data\n",
    "                \n",
    "                # Convert the particle configuration to a NumPy array and flatten it.\n",
    "                particles_flat = np.array(particles).flatten()\n",
    "                \n",
    "                writer.writerow([\n",
    "                    cycle_number,\n",
    "                    energy_per_particle,\n",
    "                    density,\n",
    "                    pressure,\n",
    "                    box_size_x,\n",
    "                    box_size_y,\n",
    "                    particles_flat.tolist()  # storing as a JSON-like list in the CSV\n",
    "                ])\n",
    "        experiment_logger.info(f\"MC run {i+1} sampled data successfully saved to {csv_filename}\")\n",
    "    except Exception as e:\n",
    "        experiment_logger.error(f\"Error: Failed to save sampled data for MC run {i+1} to {csv_filename}. Error: {e}\")\n",
    "\n",
    "    # Extract and save local samples configurations\n",
    "    configs = np.array([sample[6] for sample in mc_run.local_samples])\n",
    "    configs_filepath = os.path.join(run_folder, \"mc_run_configs.npy\")\n",
    "    np.save(configs_filepath, configs)\n",
    "    experiment_logger.info(f\"MC run {i+1} local config data saved to: {configs_filepath}\")\n",
    "\n",
    "    # Extract and save testing samples configurations\n",
    "    testing_configs = np.array(mc_run.testing_samples)\n",
    "    testing_configs_filepath = os.path.join(run_folder, \"mc_run_testing_configs.npy\") \n",
    "    np.save(testing_configs_filepath, testing_configs)\n",
    "    experiment_logger.info(f\"MC run {i+1} testing config data saved to: {testing_configs_filepath}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repo_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
