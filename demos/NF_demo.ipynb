{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NF Demo\n",
    "### Run Script to train an NF on saved MCMC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.spatial.distance import squareform, cdist\n",
    "import pandas as pd\n",
    "import os\n",
    "from demo_utils import get_project_root\n",
    "\n",
    "# Add the custom path for normflows\n",
    "project_root = get_project_root()\n",
    "nf_path = os.path.join(project_root, \"NF\")\n",
    "sys.path.append(nf_path)\n",
    "import normflows as nf\n",
    "\n",
    "# Import visualization functions from utils\n",
    "from utils import (set_icl_color_cycle, get_icl_heatmap_cmap, \n",
    "                   generate_samples, plot_frequency_heatmap, calculate_pair_correlation, \n",
    "                   plot_pair_correlation, save_rdf_data, plot_loss, plot_frequency_heatmap, \n",
    "                   plot_pair_correlation)\n",
    "\n",
    "set_icl_color_cycle()\n",
    "cmap_div = get_icl_heatmap_cmap(\"diverging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define run_training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(experiment_id, data_path, batch_size, epochs, K, n_particles, n_dimension, lr, half_box, num_samples, n_blocks, hidden_units, num_bins, weight_decay):\n",
    "    start_time = time.time()\n",
    "    print('Imports done!')\n",
    "\n",
    "    project_root = get_project_root()\n",
    "    directory = os.path.join(project_root, \"demos/data\", experiment_id)\n",
    "    if not os.path.exists(directory):\n",
    "         os.makedirs(directory)\n",
    "\n",
    "    # Set GPU if possible\n",
    "    enable_cuda = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n",
    "    print(f'device is: {device}')\n",
    "\n",
    "    # IMPORT DATA\n",
    "    npz_data = np.load(data_path)\n",
    "    df = npz_data[npz_data.files[0]]\n",
    "    # Get unique samples\n",
    "    unique_data = np.unique(df, axis=0)\n",
    "    available_samples = unique_data.shape[0]\n",
    "    print(\"\\nTotal unique samples available:\", available_samples)\n",
    "    \n",
    "    # Use the entire dataset if num_samples is not provided\n",
    "    if num_samples is None:\n",
    "         num_samples = available_samples\n",
    "         print(f\"No num_samples specified. Using all available samples: {num_samples}\")\n",
    "    elif num_samples > available_samples:\n",
    "         raise ValueError(f\"Error: Requested number of samples ({num_samples}) exceeds available unique samples ({available_samples}).\")\n",
    "    \n",
    "    # Randomly select the desired number of samples\n",
    "    indices = np.random.choice(available_samples, num_samples, replace=False)\n",
    "    unique_df = unique_data[indices]\n",
    "    unique_df = unique_df.reshape(num_samples, n_particles * n_dimension)\n",
    "    print(\"Flattened unique_df shape:\", unique_df.shape)\n",
    "    print('Data Loaded!')\n",
    "\n",
    "    def get_dataloader(data, batch_size, shuffle=True):\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "        dataset = TensorDataset(data)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        return dataloader\n",
    "\n",
    "    dataloader = get_dataloader(unique_df, batch_size=batch_size)\n",
    "    print('Data prepared!')\n",
    "\n",
    "    '''\n",
    "    Model preparation\n",
    "    '''\n",
    "    bound = half_box\n",
    "    base = nf.Energy.UniformParticle(n_particles, n_dimension, bound, device=device)\n",
    "    target = nf.Energy.SimpleLJ(n_particles * n_dimension, n_particles, 1, bound)\n",
    "    flow_layers = []\n",
    "    for i in range(K):\n",
    "        flow_layers += [nf.flows.CircularCoupledRationalQuadraticSpline(n_particles * n_dimension, n_blocks, hidden_units, \n",
    "                      range(n_particles * n_dimension), num_bins=num_bins, tail_bound=bound)]\n",
    "    model = nf.NormalizingFlow(base, flow_layers).to(device)\n",
    "    print(f'Model prepared with {n_particles} particles and {n_dimension} dimensions!')\n",
    "    setup = time.time() - start_time \n",
    "    print(f\"Time taken for setup: {setup:.2f} seconds\")\n",
    "\n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    loss_hist = np.array([])\n",
    "    loss_epoch = []\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    print(f'Optimizer prepared with learning rate: {lr} and weight decay: {weight_decay}')\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_iteration)\n",
    "    data_save = []\n",
    "\n",
    "    for it in trange(epochs, desc=\"Training Progress\"):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            # Compute loss\n",
    "            # loss,z = model.reverse_kld(num_samples)\n",
    "            loss = model.forward_kld(batch[0].to(device))\n",
    "            # Do backprop and optimizer step\n",
    "            if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # scheduler.step()\n",
    "            # Log loss\n",
    "            loss_hist = np.append(loss_hist, loss.to('cpu').data.numpy())\n",
    "            epoch_loss += loss.item()\n",
    "        loss_epoch.append(epoch_loss / len(dataloader))\n",
    "        print(f'Epoch {it+1}/{epochs}, Loss: {loss_epoch[-1]:.4f}')\n",
    "\n",
    "    loss_plot_path_svg, loss_plot_path_png = plot_loss(loss_epoch, directory)\n",
    "    print(f\"Loss plot saved successfully to: {loss_plot_path_png}\")\n",
    "    \n",
    "    model_path = os.path.join(directory, 'LJ_T1_P3_circularspline_res_dense.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved successfully to: {model_path}\")\n",
    "\n",
    "    '''\n",
    "    Test samples preparation!\n",
    "    '''\n",
    "    final_samples = generate_samples(model, n_particles, n_dimension, n_iterations=100, samples_per_iteration=500)\n",
    "    print(f'Test samples are prepared here: {np.shape(final_samples)}')\n",
    "\n",
    "    '''\n",
    "    Frequency samples\n",
    "    '''\n",
    "    heatmap_path_svg, heatmap_path_png = plot_frequency_heatmap(final_samples, directory, cmap_div, bound)\n",
    "    print(f\"Frequency heatmap saved successfully to: {heatmap_path_png}\")\n",
    "\n",
    "    '''\n",
    "    Pair correlation function (RDF)\n",
    "    '''\n",
    "    r_vals, g_r = calculate_pair_correlation(final_samples, n_particles, bound)\n",
    "    pair_corr_path_svg, pair_corr_path_png = plot_pair_correlation(r_vals, g_r, directory)\n",
    "    print(f\"Pair correlation function plot saved successfully to: {pair_corr_path_png}\")\n",
    "\n",
    "    rdf_json_path = save_rdf_data(r_vals, g_r, directory, experiment_id)\n",
    "    print(f\"RDF data saved successfully to: {rdf_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run demo training with the params varibales - edit these as desired (training with this number of samples and epochs takes roughly 5 minutes on M1 chip)\n",
    "\n",
    "\n",
    "To see plots of results go to the data folder within 'demos' and go to folder under the experiment_id name.\n",
    "\n",
    "\n",
    "To increase the resolution of the frequency heatmap \n",
    "edit the n_iterations=100, samples_per_iteration=500 arguements within the generate samples function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(project_root, \"NF/data/samples_N3_rho_0.03.npz\")\n",
    "\n",
    "params = {\n",
    "    'experiment_id': \"test_NF\",\n",
    "    'data_path': data_path,\n",
    "    'batch_size': 512,\n",
    "    'epochs': 20,\n",
    "    'K': 15,\n",
    "    'n_particles': 3,\n",
    "    'n_dimension': 2,\n",
    "    'lr': 1e-4,\n",
    "    'half_box': 5,\n",
    "    'num_samples': 10240,           # minimum 51200 samples are needed for a good RDF curve\n",
    "    'n_blocks': 8,\n",
    "    'hidden_units': 256,\n",
    "    'num_bins': 32,\n",
    "    'weight_decay': 0\n",
    "}\n",
    "\n",
    "run_training(**params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repo_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
