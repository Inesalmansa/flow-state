{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TeX rendering is available and enabled.\n",
      "TeX rendering is available and enabled.\n",
      "TeX rendering is available and enabled.\n",
      "TeX rendering is available and enabled.\n",
      "TeX rendering is available and enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 16:27:45,816 - experiment - INFO - half box is: 5.0\n",
      "2025-04-21 16:27:45,817 - experiment - INFO - Directory created at: /home/n2401517d/my_workspace/flow_state/results/algo_1_premade_102400_samples_dV_0.0_samp_fr_150\n",
      "2025-04-21 16:27:45,827 - experiment - INFO - Experiment parameters saved to /home/n2401517d/my_workspace/flow_state/results/algo_1_premade_102400_samples_dV_0.0_samp_fr_150/params.json\n",
      "2025-04-21 16:27:45,832 - experiment - INFO - run 0 starts in left\n",
      "2025-04-21 16:27:45,880 - experiment - INFO - Monte Carlo run 0 initialised and stored\n",
      "2025-04-21 16:27:45,885 - experiment - INFO - run 1 starts in right\n",
      "2025-04-21 16:27:45,887 - experiment - INFO - Monte Carlo run 1 initialised and stored\n",
      "2025-04-21 16:27:45,891 - experiment - INFO - run 2 starts in left\n",
      "2025-04-21 16:27:45,893 - experiment - INFO - Monte Carlo run 2 initialised and stored\n",
      "2025-04-21 16:27:45,897 - experiment - INFO - run 3 starts in right\n",
      "2025-04-21 16:27:45,898 - experiment - INFO - Monte Carlo run 3 initialised and stored\n",
      "2025-04-21 16:27:45,903 - experiment - INFO - run 4 starts in left\n",
      "2025-04-21 16:27:45,903 - experiment - INFO - Monte Carlo run 4 initialised and stored\n",
      "2025-04-21 16:27:45,909 - experiment - INFO - run 5 starts in right\n",
      "2025-04-21 16:27:45,910 - experiment - INFO - Monte Carlo run 5 initialised and stored\n",
      "2025-04-21 16:27:45,913 - experiment - INFO - run 6 starts in left\n",
      "2025-04-21 16:27:45,914 - experiment - INFO - Monte Carlo run 6 initialised and stored\n",
      "2025-04-21 16:27:45,918 - experiment - INFO - run 7 starts in right\n",
      "2025-04-21 16:27:45,919 - experiment - INFO - Monte Carlo run 7 initialised and stored\n",
      "2025-04-21 16:27:45,923 - experiment - INFO - run 8 starts in left\n",
      "2025-04-21 16:27:45,924 - experiment - INFO - Monte Carlo run 8 initialised and stored\n",
      "2025-04-21 16:27:45,929 - experiment - INFO - run 9 starts in right\n",
      "2025-04-21 16:27:45,929 - experiment - INFO - Monte Carlo run 9 initialised and stored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10. -10.]\n",
      "Concave potential heatmap saved to /home/n2401517d/my_workspace/flow_state/results/algo_1_premade_102400_samples_dV_0.0_samp_fr_150/double_well_concave_potential_heatmap.png\n",
      "[-10. -10.]\n",
      "[-10. -10.]\n",
      "[-10. -10.]\n",
      "Potential at x = 0: 0.0\n",
      "Potential at x = 10.0: 0.0\n",
      "Concave potential cross-section plot saved to /home/n2401517d/my_workspace/flow_state/results/algo_1_premade_102400_samples_dV_0.0_samp_fr_150/double_well_concave_potential_cross_section.png\n",
      "[-10. -10.]\n",
      "[-10. -10.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 16:27:55,691 - experiment - INFO - All Monte Carlo runs initialised\n",
      "2025-04-21 16:27:55,694 - experiment - INFO - list of run instances: [<MCMC.monte_carlo.MonteCarlo object at 0x7f54f45125d0>, <MCMC.monte_carlo.MonteCarlo object at 0x7f54f44d38d0>, <MCMC.monte_carlo.MonteCarlo object at 0x7f54f45533d0>, <MCMC.monte_carlo.MonteCarlo object at 0x7f54f45a5d90>, <MCMC.monte_carlo.MonteCarlo object at 0x7f54f4560f10>, <MCMC.monte_carlo.MonteCarlo object at 0x7f54f4561d90>, <MCMC.monte_carlo.MonteCarlo object at 0x7f54f934da50>, <MCMC.monte_carlo.MonteCarlo object at 0x7f54f45b5150>, <MCMC.monte_carlo.MonteCarlo object at 0x7f54f45a6290>, <MCMC.monte_carlo.MonteCarlo object at 0x7f54f44d09d0>]\n",
      "2025-04-21 16:28:20,006 - experiment - INFO - device is: cpu\n",
      "2025-04-21 16:28:21,499 - experiment - INFO - Total unique samples: 102400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique samples available: 204800\n",
      "Flattened global_samples_nf shape: (102400, 6)\n",
      "Data Loaded!\n",
      "(102400, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 16:28:23,956 - experiment - INFO - DataLoader details:\n",
      "2025-04-21 16:28:23,957 - experiment - INFO - Dataset size: 102400\n",
      "2025-04-21 16:28:23,958 - experiment - INFO - Number of batches: 400\n",
      "2025-04-21 16:28:30,522 - experiment - INFO - Shape of data in first batch: torch.Size([256, 6])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using network: ResidualNet(\n",
      "  (preprocessing): PeriodicFeaturesElementwise(\n",
      "    (activation): Identity()\n",
      "  )\n",
      "  (initial_layer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x ResidualBlock(\n",
      "      (activation): ReLU()\n",
      "      (batch_norm_layers): ModuleList(\n",
      "        (0-1): 2 x BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=291, bias=True)\n",
      ")\n",
      "Using network: ResidualNet(\n",
      "  (preprocessing): PeriodicFeaturesElementwise(\n",
      "    (activation): Identity()\n",
      "  )\n",
      "  (initial_layer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x ResidualBlock(\n",
      "      (activation): ReLU()\n",
      "      (batch_norm_layers): ModuleList(\n",
      "        (0-1): 2 x BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=291, bias=True)\n",
      ")\n",
      "Using network: ResidualNet(\n",
      "  (preprocessing): PeriodicFeaturesElementwise(\n",
      "    (activation): Identity()\n",
      "  )\n",
      "  (initial_layer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x ResidualBlock(\n",
      "      (activation): ReLU()\n",
      "      (batch_norm_layers): ModuleList(\n",
      "        (0-1): 2 x BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=291, bias=True)\n",
      ")\n",
      "Using network: ResidualNet(\n",
      "  (preprocessing): PeriodicFeaturesElementwise(\n",
      "    (activation): Identity()\n",
      "  )\n",
      "  (initial_layer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x ResidualBlock(\n",
      "      (activation): ReLU()\n",
      "      (batch_norm_layers): ModuleList(\n",
      "        (0-1): 2 x BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=291, bias=True)\n",
      ")\n",
      "Using network: ResidualNet(\n",
      "  (preprocessing): PeriodicFeaturesElementwise(\n",
      "    (activation): Identity()\n",
      "  )\n",
      "  (initial_layer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x ResidualBlock(\n",
      "      (activation): ReLU()\n",
      "      (batch_norm_layers): ModuleList(\n",
      "        (0-1): 2 x BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=291, bias=True)\n",
      ")\n",
      "Using network: ResidualNet(\n",
      "  (preprocessing): PeriodicFeaturesElementwise(\n",
      "    (activation): Identity()\n",
      "  )\n",
      "  (initial_layer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x ResidualBlock(\n",
      "      (activation): ReLU()\n",
      "      (batch_norm_layers): ModuleList(\n",
      "        (0-1): 2 x BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=291, bias=True)\n",
      ")\n",
      "Using network: ResidualNet(\n",
      "  (preprocessing): PeriodicFeaturesElementwise(\n",
      "    (activation): Identity()\n",
      "  )\n",
      "  (initial_layer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x ResidualBlock(\n",
      "      (activation): ReLU()\n",
      "      (batch_norm_layers): ModuleList(\n",
      "        (0-1): 2 x BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=291, bias=True)\n",
      ")\n",
      "Using network: ResidualNet(\n",
      "  (preprocessing): PeriodicFeaturesElementwise(\n",
      "    (activation): Identity()\n",
      "  )\n",
      "  (initial_layer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x ResidualBlock(\n",
      "      (activation): ReLU()\n",
      "      (batch_norm_layers): ModuleList(\n",
      "        (0-1): 2 x BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=291, bias=True)\n",
      ")\n",
      "Using network: ResidualNet(\n",
      "  (preprocessing): PeriodicFeaturesElementwise(\n",
      "    (activation): Identity()\n",
      "  )\n",
      "  (initial_layer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x ResidualBlock(\n",
      "      (activation): ReLU()\n",
      "      (batch_norm_layers): ModuleList(\n",
      "        (0-1): 2 x BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=291, bias=True)\n",
      ")\n",
      "Using network: ResidualNet(\n",
      "  (preprocessing): PeriodicFeaturesElementwise(\n",
      "    (activation): Identity()\n",
      "  )\n",
      "  (initial_layer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x ResidualBlock(\n",
      "      (activation): ReLU()\n",
      "      (batch_norm_layers): ModuleList(\n",
      "        (0-1): 2 x BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=291, bias=True)\n",
      ")\n",
      "Using network: ResidualNet(\n",
      "  (preprocessing): PeriodicFeaturesElementwise(\n",
      "    (activation): Identity()\n",
      "  )\n",
      "  (initial_layer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x ResidualBlock(\n",
      "      (activation): ReLU()\n",
      "      (batch_norm_layers): ModuleList(\n",
      "        (0-1): 2 x BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=291, bias=True)\n",
      ")\n",
      "Using network: ResidualNet(\n",
      "  (preprocessing): PeriodicFeaturesElementwise(\n",
      "    (activation): Identity()\n",
      "  )\n",
      "  (initial_layer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x ResidualBlock(\n",
      "      (activation): ReLU()\n",
      "      (batch_norm_layers): ModuleList(\n",
      "        (0-1): 2 x BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=291, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 16:28:33,718 - experiment - INFO - Model prepared with 3 particles and 2 dimensions!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using network: ResidualNet(\n",
      "  (preprocessing): PeriodicFeaturesElementwise(\n",
      "    (activation): Identity()\n",
      "  )\n",
      "  (initial_layer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x ResidualBlock(\n",
      "      (activation): ReLU()\n",
      "      (batch_norm_layers): ModuleList(\n",
      "        (0-1): 2 x BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=291, bias=True)\n",
      ")\n",
      "Using network: ResidualNet(\n",
      "  (preprocessing): PeriodicFeaturesElementwise(\n",
      "    (activation): Identity()\n",
      "  )\n",
      "  (initial_layer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x ResidualBlock(\n",
      "      (activation): ReLU()\n",
      "      (batch_norm_layers): ModuleList(\n",
      "        (0-1): 2 x BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=291, bias=True)\n",
      ")\n",
      "Using network: ResidualNet(\n",
      "  (preprocessing): PeriodicFeaturesElementwise(\n",
      "    (activation): Identity()\n",
      "  )\n",
      "  (initial_layer): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x ResidualBlock(\n",
      "      (activation): ReLU()\n",
      "      (batch_norm_layers): ModuleList(\n",
      "        (0-1): 2 x BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=291, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 16:31:31,111 - experiment - INFO - Optimizer prepared with learning rate: 0.0001\n",
      "Training Progress:   1%|          | 1/100 [22:45<37:33:01, 1365.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 310\u001b[0m\n\u001b[1;32m    307\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# loss,z = model.reverse_kld(num_samples)\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward_kld(batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Do backprop and optimizer step\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# if ~(torch.isnan(loss) | torch.isinf(loss)):\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(loss):\n",
      "File \u001b[0;32m~/my_workspace/flow_state/NF/normflows/core.py:100\u001b[0m, in \u001b[0;36mNormalizingFlow.forward_kld\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m z \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflows) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 100\u001b[0m     z, log_det \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflows[i]\u001b[38;5;241m.\u001b[39minverse(z)\n\u001b[1;32m    101\u001b[0m     log_q \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m log_det\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# log_q += self.q0.log_prob(z)\u001b[39;00m\n",
      "File \u001b[0;32m~/my_workspace/flow_state/NF/normflows/flows/neural_spline/wrapper.py:274\u001b[0m, in \u001b[0;36mCircularCoupledRationalQuadraticSpline.inverse\u001b[0;34m(self, z, context)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, z, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 274\u001b[0m     z, log_det \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprqct(z, context)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z, log_det\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/my_workspace/flow_state/NF/normflows/flows/neural_spline/coupling.py:86\u001b[0m, in \u001b[0;36mCoupling.forward\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     83\u001b[0m identity_split \u001b[38;5;241m=\u001b[39m inputs[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midentity_features, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m     84\u001b[0m transform_split \u001b[38;5;241m=\u001b[39m inputs[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_features, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m---> 86\u001b[0m transform_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_net(identity_split, context)\n\u001b[1;32m     87\u001b[0m transform_split, logabsdet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coupling_transform_forward(\n\u001b[1;32m     88\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mtransform_split, transform_params\u001b[38;5;241m=\u001b[39mtransform_params\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munconditional_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/my_workspace/flow_state/NF/normflows/nets/resnet.py:102\u001b[0m, in \u001b[0;36mResidualNet.forward\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m    100\u001b[0m     temps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_layer(torch\u001b[38;5;241m.\u001b[39mcat((temps, context), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 102\u001b[0m     temps \u001b[38;5;241m=\u001b[39m block(temps, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    103\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(temps)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/my_workspace/flow_state/NF/normflows/nets/resnet.py:45\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batch_norm:\n\u001b[1;32m     44\u001b[0m     temps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm_layers[\u001b[38;5;241m1\u001b[39m](temps)\n\u001b[0;32m---> 45\u001b[0m temps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(temps)\n\u001b[1;32m     46\u001b[0m temps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(temps)\n\u001b[1;32m     47\u001b[0m temps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_layers[\u001b[38;5;241m1\u001b[39m](temps)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py:104\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:1500\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from scipy.spatial.distance import squareform, cdist\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from utils import (get_project_root, setup_logger, get_dataloader, calculate_well_statistics, \n",
    "                  set_icl_color_cycle, get_icl_heatmap_cmap, plot_loss, \n",
    "                  plot_frequency_heatmap, generate_samples, \n",
    "                  calculate_pair_correlation, plot_pair_correlation, save_rdf_data, plot_acceptance_rate,\n",
    "                  plot_avg_free_energy, plot_well_statistics, plot_avg_x_coordinate,\n",
    "                  plot_multiple_avg_x_coordinates)\n",
    "import argparse\n",
    "import csv\n",
    "\n",
    "set_icl_color_cycle()\n",
    "cmap_div = get_icl_heatmap_cmap(\"diverging\")\n",
    "\n",
    "# importing the NF and MC modules\n",
    "project_root = get_project_root()\n",
    "nf_path = os.path.join(project_root, \"NF\")\n",
    "sys.path.append(nf_path)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "import normflows as NF\n",
    "import MCMC as MC\n",
    "\n",
    "# EXPERIMENT VARIABLES\n",
    "NUM_MC_RUNS = 10        # how many mc run running in 'parallel'\n",
    "MASTER_SEED = 42        # master seed to give seeds for each mc run to run differently\n",
    "NUM_PARTICLES = 3\n",
    "NUM_DIM = 2\n",
    "NUM_TRAINING_CYCLES = 0\n",
    "\n",
    "# mc needed params\n",
    "TEMP = 1.0\n",
    "RHO = 0.03\n",
    "ASPECT_RATIO = 1.0\n",
    "VISUALISE = True\n",
    "CHECKING = True\n",
    "EQUILIBRATION_STEPS = 5000\n",
    "NUM_WELLS = 2\n",
    "V0_LIST = [-10.0,-10.0]              # depth of wells\n",
    "R0 = 1.2                # radius of bottom of well\n",
    "K_VAL = 15              # steepness of well slopes\n",
    "HALF_BOX = ((NUM_PARTICLES/RHO)**(1/NUM_DIM))/2\n",
    "INITIAL_MAX_DISPLACEMENT = 0.65\n",
    "SAMPLING_FREQUENCY = 150\n",
    "ADJUSTING_FREQUENCY = 5000\n",
    "\n",
    "# nf needed params\n",
    "INITIAL_TRAINING_NUM_SAMPLES = 102400\n",
    "BATCH_SIZE = 256\n",
    "K = 15\n",
    "EPOCHS = 100\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 0\n",
    "HIDDEN_UNITS = 256\n",
    "NUM_BINS = 32\n",
    "N_BLOCKS = 8\n",
    "\n",
    "# post training runs parameters\n",
    "TESTING = True\n",
    "BIG_MOVE_ATTEMPTS = 1000\n",
    "BIG_MOVE_INTERVAL = 1000\n",
    "NUM_SAMPLES_FOR_ANALYSIS = BIG_MOVE_ATTEMPTS * NUM_MC_RUNS\n",
    "\n",
    "experiment_name = 'algo_1_premade_102400_samples_dV_0.0_samp_fr_150'\n",
    "\n",
    "# result saving\n",
    "# parser = argparse.ArgumentParser(add_help=False)\n",
    "# parser.add_argument(\"--experiment_id\", type=str, required=True, help=\"Unique identifier for the experiment\")\n",
    "# args, _ = parser.parse_known_args()\n",
    "# experiment_name = args.experiment_id\n",
    "\n",
    "directory = os.path.join(get_project_root(), \"results\", experiment_name)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "# Setup a global experiment logger that logs to a file in the experiment directory.\n",
    "experiment_log_file = os.path.join(directory, \"experiment.log\")\n",
    "experiment_logger = setup_logger(\"experiment\", experiment_log_file, file_level=logging.DEBUG, stream_level=logging.INFO)\n",
    "experiment_logger.info(\"half box is: \" + str(HALF_BOX))\n",
    "experiment_logger.info(f\"Directory created at: {directory}\")\n",
    "\n",
    "mc_runs_directory = os.path.join(directory, \"mc_runs\")\n",
    "os.makedirs(mc_runs_directory, exist_ok=True)\n",
    "training_rounds_directory = os.path.join(directory, \"training_rounds\")\n",
    "os.makedirs(training_rounds_directory, exist_ok=True)\n",
    "\n",
    "# Save experiment parameters to a JSON file in the parent experiment directory\n",
    "experiment_params = {\n",
    "    \"NUM_MC_RUNS\": NUM_MC_RUNS,\n",
    "    \"MASTER_SEED\": MASTER_SEED,\n",
    "    \"NUM_PARTICLES\": NUM_PARTICLES,\n",
    "    \"NUM_DIM\": NUM_DIM,\n",
    "    \"NUM_TRAINING_CYCLES\": NUM_TRAINING_CYCLES,\n",
    "    \"INITIAL_TRAINING_NUM_SAMPLES\": INITIAL_TRAINING_NUM_SAMPLES,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"K\": K,\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"LR\": LR,\n",
    "    \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
    "    \"HIDDEN_UNITS\": HIDDEN_UNITS,\n",
    "    \"NUM_BINS\": NUM_BINS,\n",
    "    \"N_BLOCKS\": N_BLOCKS,\n",
    "    \"TEMP\": TEMP,\n",
    "    \"RHO\": RHO,\n",
    "    \"ASPECT_RATIO\": ASPECT_RATIO,\n",
    "    \"VISUALISE\": VISUALISE,\n",
    "    \"CHECKING\": CHECKING,\n",
    "    \"EQUILIBRATION_STEPS\": EQUILIBRATION_STEPS,\n",
    "    \"NUM_WELLS\": NUM_WELLS,\n",
    "    \"V0_LIST\": V0_LIST,\n",
    "    \"R0\": R0,\n",
    "    \"K_VAL\": K_VAL,\n",
    "    \"HALF_BOX\": HALF_BOX,\n",
    "    \"INITIAL_MAX_DISPLACEMENT\": INITIAL_MAX_DISPLACEMENT,\n",
    "    \"SAMPLING_FREQUENCY\": SAMPLING_FREQUENCY,\n",
    "    \"ADJUSTING_FREQUENCY\": ADJUSTING_FREQUENCY,\n",
    "    \"TESTING\": TESTING,\n",
    "    \"BIG_MOVE_ATTEMPTS\": BIG_MOVE_ATTEMPTS,\n",
    "    \"BIG_MOVE_INTERVAL\": BIG_MOVE_INTERVAL,\n",
    "    \"NUM_SAMPLES_FOR_ANALYSIS\": NUM_SAMPLES_FOR_ANALYSIS\n",
    "}\n",
    "json_file_path = os.path.join(directory, \"params.json\")\n",
    "with open(json_file_path, \"w\") as f:\n",
    "    json.dump(experiment_params, f, indent=4)\n",
    "experiment_logger.info(f\"Experiment parameters saved to {json_file_path}\")\n",
    "\n",
    "# initialisation of experiment - sets up the monte carlo runs and equilibrates and collects samples\n",
    "mc_runs = []  # list to store each Monte Carlo simulation instance\n",
    "for i in range(NUM_MC_RUNS):\n",
    "    seed = i + MASTER_SEED\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create a dedicated folder and logger for this Monte Carlo run.\n",
    "    run_dir = os.path.join(mc_runs_directory, f\"run_{i+1:03d}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    mc_log_file = os.path.join(run_dir, \"mc_run.log\")\n",
    "    mc_logger = setup_logger(f\"MC_run_{i+1:03d}\", mc_log_file, file_level=logging.DEBUG, stream_level=logging.WARNING)\n",
    "\n",
    "    # even i runs initialise left and odd initialise right\n",
    "    if i % 2 == 0:\n",
    "        particles, sim_box = MC.initialise_low_left(\n",
    "                num_particles=NUM_PARTICLES,\n",
    "                rho=RHO,\n",
    "                aspect_ratio=ASPECT_RATIO,\n",
    "                visualise=False,\n",
    "                checking=False\n",
    "            )\n",
    "        experiment_logger.info(f\"run {i} starts in left\")\n",
    "    else:\n",
    "        particles, sim_box = MC.initialise_low_right(\n",
    "                num_particles=NUM_PARTICLES,\n",
    "                rho=RHO,\n",
    "                aspect_ratio=ASPECT_RATIO,\n",
    "                visualise=False,\n",
    "                checking=False\n",
    "            )\n",
    "        experiment_logger.info(f\"run {i} starts in right\")\n",
    "        \n",
    "    mc_run = MC.MonteCarlo(\n",
    "        particles=particles,\n",
    "        sim_box=sim_box,\n",
    "        temperature=TEMP,\n",
    "        num_particles=NUM_PARTICLES,\n",
    "        num_wells=NUM_WELLS,    # Pass number of wells\n",
    "        V0_list=V0_LIST,                  # Pass well depth\n",
    "        r0=R0,\n",
    "        k=K_VAL,\n",
    "        initial_max_displacement=INITIAL_MAX_DISPLACEMENT,  # Adjust as needed\n",
    "        target_acceptance=0.5,\n",
    "        timing=False,\n",
    "        checking=CHECKING,\n",
    "        logger=mc_logger,       # Each run now logs to its own file\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    mc_runs.append(mc_run)\n",
    "    experiment_logger.info(f\"Monte Carlo run {i} initialised and stored\")\n",
    "\n",
    "# Plot the potential wells for visualization\n",
    "MC.visualise.plot_potential(\n",
    "    box_siz_x=sim_box.box_size_x,\n",
    "    box_size_y=sim_box.box_size_y,\n",
    "    V0_list=V0_LIST,\n",
    "    r0=R0,\n",
    "    k=K_VAL,\n",
    "    num_wells=NUM_WELLS,\n",
    "    output_path=directory\n",
    ")\n",
    "\n",
    "experiment_logger.info(\"All Monte Carlo runs initialised\")\n",
    "experiment_logger.info(\"list of run instances: \" + str(mc_runs))\n",
    "\n",
    "# equilibration of each mc run\n",
    "for mc_run in mc_runs:\n",
    "    for step in range(1, EQUILIBRATION_STEPS + 1):\n",
    "        mc_run.particle_displacement()\n",
    "        if step % ADJUSTING_FREQUENCY == 0:\n",
    "            mc_run.adjust_displacement()\n",
    "        if step % SAMPLING_FREQUENCY == 0:\n",
    "            sample = mc_run.sample(step)\n",
    "            mc_run.local_samples.append(sample)\n",
    "    \n",
    "# Plot and save the equilibrated configuration for each MC run\n",
    "for i, mc_run in enumerate(mc_runs):\n",
    "    run_dir = os.path.join(mc_runs_directory, f\"run_{i+1:03d}\")\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    current_config = mc_run.particles\n",
    "    plt.scatter(current_config[:, 0], current_config[:, 1], alpha=0.6)\n",
    "    plt.xlim(0, 2*HALF_BOX)\n",
    "    plt.ylim(0, 2*HALF_BOX)\n",
    "    plt.xlabel(r'$x$')\n",
    "    plt.ylabel(r'$y$') \n",
    "    plt.title(f'Equilibrated Configuration - MC Run {i+1}')\n",
    "    \n",
    "    # Save plot to the MC run's directory\n",
    "    config_plot_path = os.path.join(run_dir, f\"equilibrated_config.png\")\n",
    "    fig.savefig(config_plot_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    mc_run.logger.info(f\"Equilibrated configuration plot saved to: {config_plot_path}\")\n",
    "\n",
    "# Initialize MCMC step counter right after equilibration\n",
    "total_mcmc_steps = 0\n",
    "\n",
    "# Initialize acceptance rate tracking at the very beginning\n",
    "p_acc_history = [0.0]  # Initial acceptance rate is 0\n",
    "mcmc_steps_history = [0]  # Start from MCMC step 0\n",
    "big_move_attempts = 0\n",
    "big_move_accepts = 0\n",
    "\n",
    "# prepping initial samples as for dataloader\n",
    "enable_cuda = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n",
    "experiment_logger.info('device is: ' + str(device))\n",
    "\n",
    "num_samples = 102400\n",
    "data_path = \"/home/n2401517d/my_workspace/flow_state/NF/data/samples_N3_rho_0.03.npz\"\n",
    "\n",
    "# IMPORT DATA\n",
    "npz_data = np.load(data_path)\n",
    "df = npz_data[npz_data.files[0]]\n",
    "# Get unique samples\n",
    "unique_data = np.unique(df, axis=0)\n",
    "available_samples = unique_data.shape[0]\n",
    "print(\"\\nTotal unique samples available:\", available_samples)\n",
    "\n",
    "# Use the entire dataset if num_samples is not provided\n",
    "if num_samples is None:\n",
    "        num_samples = available_samples\n",
    "        print(f\"No num_samples specified. Using all available samples: {num_samples}\")\n",
    "elif num_samples > available_samples:\n",
    "        raise ValueError(f\"Error: Requested number of samples ({num_samples}) exceeds available unique samples ({available_samples}).\")\n",
    "\n",
    "# Randomly select the desired number of samples\n",
    "indices = np.random.choice(available_samples, num_samples, replace=False)\n",
    "global_samples_nf = unique_data[indices]\n",
    "global_samples_nf = global_samples_nf.reshape(num_samples, NUM_PARTICLES * NUM_DIM)\n",
    "print(\"Flattened global_samples_nf shape:\", global_samples_nf.shape)\n",
    "print('Data Loaded!')\n",
    "\n",
    "unique_data = np.unique(global_samples_nf, axis=0)\n",
    "experiment_logger.info(\"Total unique samples: \" + str(unique_data.shape[0]))\n",
    "\n",
    "dataloader = get_dataloader(global_samples_nf, NUM_PARTICLES, NUM_DIM, device, batch_size=BATCH_SIZE, shuffle=True)\n",
    "experiment_logger.info(\"DataLoader details:\")\n",
    "experiment_logger.info(\"Dataset size: \" + str(len(dataloader.dataset)))\n",
    "experiment_logger.info(\"Number of batches: \" + str(len(dataloader)))\n",
    "first_batch = next(iter(dataloader))\n",
    "experiment_logger.info(\"Shape of data in first batch: \" + str(first_batch[0].shape))\n",
    "\n",
    "# initialise the normalizing flow model\n",
    "bound = HALF_BOX\n",
    "base = NF.Energy.UniformParticle(NUM_PARTICLES, NUM_DIM, bound, device=device)\n",
    "# target = NF.Energy.SimpleLJ(NUM_PARTICLES * NUM_DIM, NUM_PARTICLES, 1, bound)    # not used for now\n",
    "# K = 15 - now defined in the argument of function\n",
    "flow_layers = []\n",
    "for i in range(K):\n",
    "    flow_layers += [NF.flows.CircularCoupledRationalQuadraticSpline(NUM_PARTICLES * NUM_DIM, NUM_BINS, HIDDEN_UNITS,\n",
    "                    range(NUM_PARTICLES * NUM_DIM), num_bins=NUM_BINS, tail_bound=bound)]\n",
    "model = NF.NormalizingFlow(base, flow_layers).to(device)\n",
    "experiment_logger.info(f'Model prepared with {NUM_PARTICLES} particles and {NUM_DIM} dimensions!')\n",
    "\n",
    "# Create a dedicated folder and logger for the first training round of the normalizing flow.\n",
    "nf_training_dir = os.path.join(training_rounds_directory, \"initial_training_round\")\n",
    "os.makedirs(nf_training_dir, exist_ok=True)\n",
    "nf_log_file = os.path.join(nf_training_dir, \"nf_training_round.log\")\n",
    "nf_logger = setup_logger(\"NF_training_round\", nf_log_file, file_level=logging.DEBUG, stream_level=logging.WARNING)\n",
    "\n",
    "# train the normalizing flow on initial training batch\n",
    "loss_hist = np.array([])\n",
    "loss_epoch = []\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "experiment_logger.info(f'Optimizer prepared with learning rate: {LR}')\n",
    "data_save = []\n",
    "\n",
    "for it in trange(EPOCHS, desc=\"Training Progress\"):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # Compute loss\n",
    "        # loss,z = model.reverse_kld(num_samples)\n",
    "        loss = model.forward_kld(batch[0].to(device))\n",
    "        # Do backprop and optimizer step\n",
    "        # if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            experiment_logger.warning(\"Warning: Loss is NaN or Inf. Skipping this batch.\")\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "        # Log loss\n",
    "        loss_hist = np.append(loss_hist, loss.to('cpu').data.numpy())\n",
    "        epoch_loss += loss.item()\n",
    "    loss_epoch.append(epoch_loss / len(dataloader))\n",
    "    nf_logger.info(f'Epoch {it+1}/{EPOCHS}, Loss: {loss_epoch[-1]:.4f}')\n",
    "\n",
    "# Plot loss\n",
    "loss_plot_path_svg, loss_plot_path_png = plot_loss(loss_epoch, nf_training_dir)\n",
    "nf_logger.info(f\"Loss plot saved successfully to: {loss_plot_path_png}\")\n",
    "\n",
    "model_path = os.path.join(nf_training_dir, \"initial_model_circularspline_res_dense.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "nf_logger.info(f\"Model saved successfully to: {model_path}\")\n",
    "\n",
    "# change to eval mode to test the NF model\n",
    "model.eval()\n",
    "samples_for_mc = []\n",
    "z = model.sample(NUM_MC_RUNS)\n",
    "z_ = z.reshape(-1, NUM_PARTICLES, NUM_DIM)  # Reshape as required\n",
    "z_ = z_.to('cpu').detach().numpy()  # Move to CPU and convert to NumPy\n",
    "z_ = z_ + HALF_BOX  # Add HALF_BOX to each element in the samples\n",
    "print(z_)\n",
    "\n",
    "# Generate samples\n",
    "a_ = generate_samples(model, NUM_PARTICLES, NUM_DIM, \n",
    "                     n_iterations=NUM_SAMPLES_FOR_ANALYSIS // 5000 + 1, \n",
    "                     samples_per_iteration=5000)\n",
    "a_ = a_ + HALF_BOX  # Add HALF_BOX to each element in the samples\n",
    "print(a_.shape)\n",
    "samples_path = os.path.join(nf_training_dir, \"samples.npy\")\n",
    "np.save(samples_path, a_)\n",
    "final_model_samples = a_\n",
    "\n",
    "# make heatmap\n",
    "heatmap_path_svg, heatmap_path_png = plot_frequency_heatmap(a_- HALF_BOX, nf_training_dir, cmap_div, HALF_BOX)\n",
    "nf_logger.info(f\"Frequency heatmap saved successfully to: {heatmap_path_png}\")\n",
    "\n",
    "# Calculate and plot pair correlation function\n",
    "r_vals, g_r = calculate_pair_correlation(a_ - HALF_BOX, NUM_PARTICLES, HALF_BOX, dr=HALF_BOX/50)\n",
    "pair_corr_path_svg, pair_corr_path_png = plot_pair_correlation(\n",
    "    r_vals, \n",
    "    g_r, \n",
    "    nf_training_dir + \"/\"\n",
    ")\n",
    "print(f\"Pair correlation function plot saved successfully to: {pair_corr_path_svg} and {pair_corr_path_png}\")\n",
    "\n",
    "# Add another data point showing 0 acceptance at the end of training\n",
    "p_acc_history.append(0.0)\n",
    "mcmc_steps_history.append(total_mcmc_steps)\n",
    "\n",
    "# -------------------------\n",
    "# After the full experiment, running the mc_runs with the samples from the final trained model\n",
    "# -------------------------\n",
    "trained_nf_model = model  # this is your trained normalizing flow model\n",
    "\n",
    "# For each Monte Carlo simulation instance, assign the trained model:\n",
    "for mc_run in mc_runs:\n",
    "    mc_run.set_nf_model(trained_nf_model)\n",
    "\n",
    "\n",
    "if TESTING:\n",
    "    test_configs = final_model_samples  # configurations produced by the final model for big move suggestions\n",
    "    \n",
    "    free_energy_array = []\n",
    "    \n",
    "    # Initialize arrays to store acceptance rates for each run\n",
    "    run_acceptance_rates = []\n",
    "    run_acceptance_histories = []  # To store acceptance rate history for each run\n",
    "    run_mcmc_steps_histories = []  # To store MCMC steps history for each run\n",
    "\n",
    "    # Testing phase: for each MC run, run displacement steps and suggest big moves\n",
    "    for run_idx, mc_run in enumerate(mc_runs):\n",
    "        run_big_move_accepts = 0\n",
    "        run_acceptance_history = []\n",
    "        run_mcmc_steps_history = []\n",
    "        local_mcmc_steps = 0\n",
    "        \n",
    "        for attempt in range(BIG_MOVE_ATTEMPTS):\n",
    "            # Run BIG_MOVE_INTERVAL displacement steps and collect testing samples\n",
    "            for step in range(1, BIG_MOVE_INTERVAL + 1):\n",
    "                mc_run.particle_displacement()\n",
    "                local_mcmc_steps += 1\n",
    "                if step % SAMPLING_FREQUENCY == 0:\n",
    "                    sample = mc_run.sample(step)\n",
    "                    mc_run.local_samples.append(sample)\n",
    "                    mc_run.testing_samples.append(sample[6])\n",
    "            \n",
    "            # Each MC run receives a unique configuration for a big move suggestion\n",
    "            test_config = test_configs[attempt * len(mc_runs) + run_idx]\n",
    "            accepted = mc_run.nf_big_move(test_config)\n",
    "            \n",
    "            if accepted:\n",
    "                run_big_move_accepts += 1\n",
    "            \n",
    "            # Calculate and store current acceptance rate for this run\n",
    "            current_run_acc_rate = run_big_move_accepts / (attempt + 1)\n",
    "            run_acceptance_history.append(current_run_acc_rate)\n",
    "            run_mcmc_steps_history.append(local_mcmc_steps)\n",
    "            \n",
    "            experiment_logger.info(f\"Run {run_idx+1}, Attempt {attempt+1}: \"\n",
    "                                 f\"p_accept = {current_run_acc_rate:.4f} \"\n",
    "                                 f\"({run_big_move_accepts}/{attempt + 1})\")\n",
    "\n",
    "        # Store final acceptance rate for this run\n",
    "        final_run_acc_rate = run_big_move_accepts / BIG_MOVE_ATTEMPTS\n",
    "        run_acceptance_rates.append(final_run_acc_rate)\n",
    "        run_acceptance_histories.append(run_acceptance_history)\n",
    "        run_mcmc_steps_histories.append(run_mcmc_steps_history)\n",
    "        \n",
    "        print(f\"Testing Summary - Simulation run {run_idx+1}: \"\n",
    "              f\"Final p_accept = {final_run_acc_rate:.4f} \"\n",
    "              f\"({run_big_move_accepts}/{BIG_MOVE_ATTEMPTS} moves accepted)\")\n",
    "\n",
    "    # Calculate average acceptance rate and error across all runs\n",
    "    mean_acceptance = np.mean(run_acceptance_rates)\n",
    "    std_acceptance = np.std(run_acceptance_rates)\n",
    "    sem_acceptance = std_acceptance / np.sqrt(len(run_acceptance_rates))\n",
    "    \n",
    "    experiment_logger.info(f\"\\nOverall acceptance rate statistics:\")\n",
    "    experiment_logger.info(f\"Mean acceptance rate: {mean_acceptance:.4f}\")\n",
    "    experiment_logger.info(f\"Standard deviation: {std_acceptance:.4f}\")\n",
    "    experiment_logger.info(f\"Standard error of mean: {sem_acceptance:.4f}\")\n",
    "\n",
    "    # Plot individual run acceptance histories\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for run_idx, history in enumerate(run_acceptance_histories):\n",
    "        plt.plot(run_mcmc_steps_histories[run_idx], \n",
    "                history, \n",
    "                alpha=0.3, \n",
    "                label=f'Run {run_idx+1}')\n",
    "    \n",
    "    # Plot mean acceptance rate with error bands\n",
    "    mean_history = np.mean(run_acceptance_histories, axis=0)\n",
    "    std_history = np.std(run_acceptance_histories, axis=0)\n",
    "    mcmc_steps = run_mcmc_steps_histories[0]  # All runs have same MCMC steps\n",
    "    \n",
    "    plt.plot(mcmc_steps, mean_history, 'k-', label='Mean', linewidth=2)\n",
    "    plt.fill_between(mcmc_steps, \n",
    "                     mean_history - std_history, \n",
    "                     mean_history + std_history, \n",
    "                     color='gray', \n",
    "                     alpha=0.2, \n",
    "                     label='±1 std')\n",
    "    \n",
    "    plt.xlabel('MCMC Steps')\n",
    "    plt.ylabel('Acceptance Rate')\n",
    "    plt.title('Big Move Acceptance Rate by Run')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the plot\n",
    "    acc_plot_path_svg = os.path.join(directory, \"acceptance_rates_by_run.svg\")\n",
    "    acc_plot_path_png = os.path.join(directory, \"acceptance_rates_by_run.png\")\n",
    "    plt.savefig(acc_plot_path_svg, bbox_inches='tight')\n",
    "    plt.savefig(acc_plot_path_png, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    experiment_logger.info(f\"Acceptance rate plots saved to: {acc_plot_path_png}\")\n",
    "    \n",
    "    # Save the acceptance rate data to a CSV file\n",
    "    acceptance_data_path = os.path.join(directory, \"acceptance_rate_data.csv\")\n",
    "    with open(acceptance_data_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['MCMC_Steps'] + [f'Run_{i+1}' for i in range(len(mc_runs))] + ['Mean', 'Std'])\n",
    "        for step_idx in range(len(mcmc_steps)):\n",
    "            row = [mcmc_steps[step_idx]]\n",
    "            row.extend([history[step_idx] for history in run_acceptance_histories])\n",
    "            row.extend([mean_history[step_idx], std_history[step_idx]])\n",
    "            writer.writerow(row)\n",
    "    experiment_logger.info(f\"Acceptance rate data saved to: {acceptance_data_path}\")\n",
    "\n",
    "# if TESTING:\n",
    "#     test_configs = final_model_samples  # configurations produced by the final model for big move suggestions\n",
    "    \n",
    "#     free_energy_array = []\n",
    "\n",
    "#     # Testing phase: for each MC run, run displacement steps and suggest a big move with a unique config for each attempt\n",
    "#     for run_idx, mc_run in enumerate(mc_runs):\n",
    "#         for attempt in range(BIG_MOVE_ATTEMPTS):\n",
    "#             # Run BIG_MOVE_INTERVAL displacement steps and collect testing samples\n",
    "#             for step in range(1, BIG_MOVE_INTERVAL + 1):\n",
    "#                 mc_run.particle_displacement()\n",
    "#                 total_mcmc_steps += 1\n",
    "#                 if step % ADJUSTING_FREQUENCY == 0:\n",
    "#                     mc_run.adjust_displacement()\n",
    "#                 if step % SAMPLING_FREQUENCY == 0:\n",
    "#                     sample = mc_run.sample(step)\n",
    "#                     mc_run.local_samples.append(sample)\n",
    "#                     mc_run.testing_samples.append(sample[6])  # store configuration (in MC box coordinates)\n",
    "            \n",
    "#             # Each MC run receives a unique configuration for a big move suggestion\n",
    "#             test_config = test_configs[attempt * len(mc_runs) + run_idx]\n",
    "#             # Assume nf_big_move returns a boolean indicating whether the big move was accepted\n",
    "#             accepted = mc_run.nf_big_move(test_config)\n",
    "#             big_move_attempts += 1\n",
    "#             if accepted:\n",
    "#                 big_move_accepts += 1\n",
    "            \n",
    "#             # Initialize testing move counters if they don't exist yet\n",
    "#             if not hasattr(mc_run, 'test_moves_attempted'):\n",
    "#                 mc_run.test_moves_attempted = 0\n",
    "#                 mc_run.test_moves_accepted = 0\n",
    "            \n",
    "#             # Update testing counters for this simulation run\n",
    "#             mc_run.test_moves_attempted += 1\n",
    "#             if accepted:\n",
    "#                 mc_run.test_moves_accepted += 1\n",
    "            \n",
    "#             # Calculate and print the acceptance probability for this simulation run so far\n",
    "#             p_accept = mc_run.test_moves_accepted / mc_run.test_moves_attempted\n",
    "            \n",
    "#             # Track global acceptance rate after each big move attempt\n",
    "#             current_global_p_acc = big_move_accepts / big_move_attempts\n",
    "#             p_acc_history.append(current_global_p_acc)\n",
    "#             mcmc_steps_history.append(total_mcmc_steps)\n",
    "                \n",
    "#             experiment_logger.info(f\"Step {total_mcmc_steps}, Run {run_idx+1}, Attempt {attempt+1}: \"\n",
    "#                                     f\"p_accept = {current_global_p_acc:.4f} ({big_move_accepts}/{big_move_attempts})\")\n",
    "\n",
    "#         # Print summary after all attempts for this run are complete\n",
    "#         print(f\"Testing Summary - Simulation run {run_idx+1}: Final p_accept = {p_accept:.4f} ({mc_run.test_moves_accepted}/{mc_run.test_moves_attempted} moves accepted)\")\n",
    "\n",
    "#     # Plot the acceptance rate history\n",
    "#     acc_plot_path_svg, acc_plot_path_png = plot_acceptance_rate(\n",
    "#         p_acc_history, \n",
    "#         directory, \n",
    "#         x_values=mcmc_steps_history, \n",
    "#         xlabel='MCMC Steps',\n",
    "#         base_filename=\"nf_acceptance_rate\",\n",
    "#         color='C2'\n",
    "#     )\n",
    "#     experiment_logger.info(f\"Acceptance rate plot saved to: {acc_plot_path_png}\")\n",
    "    \n",
    "#     # Save the acceptance rate data to a CSV file\n",
    "#     acceptance_data_path = os.path.join(directory, \"acceptance_rate_data.csv\")\n",
    "#     with open(acceptance_data_path, 'w', newline='') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "#         writer.writerow(['MCMC_Steps', 'Acceptance_Rate'])\n",
    "#         for steps, acc_rate in zip(mcmc_steps_history, p_acc_history):\n",
    "#             writer.writerow([steps, acc_rate])\n",
    "#     experiment_logger.info(f\"Acceptance rate data saved to: {acceptance_data_path}\")\n",
    "\n",
    "for run_idx, mc_run in enumerate(mc_runs):\n",
    "    if mc_run.testing_samples:\n",
    "        testing_configs = np.array(mc_run.testing_samples)  # shape: (num_samples, num_particles, 2)\n",
    "        testing_configs = testing_configs[:]\n",
    "        \n",
    "        # Compute well statistics\n",
    "        avg_x_values, p_a_values, p_b_values, deltaF_normalized_values, runs = calculate_well_statistics(\n",
    "            testing_configs, 0, HALF_BOX, R0\n",
    "        )\n",
    "\n",
    "        # Add this run's free energy data to the array\n",
    "        free_energy_array.append(deltaF_normalized_values)\n",
    "\n",
    "        # Plot well statistics\n",
    "        run_folder = os.path.join(mc_runs_directory, f\"run_{run_idx+1:03d}\")\n",
    "        well_stats_path_svg, well_stats_path_png = plot_well_statistics(\n",
    "            avg_x_values, \n",
    "            p_a_values, \n",
    "            p_b_values, \n",
    "            deltaF_normalized_values, \n",
    "            runs, \n",
    "            HALF_BOX,\n",
    "            run_folder\n",
    "        )\n",
    "        \n",
    "        # Plot average x coordinate for individual particles\n",
    "        avg_x_path_svg, avg_x_path_png = plot_avg_x_coordinate(\n",
    "            testing_configs,\n",
    "            run_folder,\n",
    "            run_idx+1\n",
    "        )\n",
    "\n",
    "# Plot summary of the first 10 MC runs if there are at least 10\n",
    "if len(mc_runs) >= 10:\n",
    "    multi_avg_x_path_svg, multi_avg_x_path_png = plot_multiple_avg_x_coordinates(\n",
    "        mc_runs,\n",
    "        directory\n",
    "    )\n",
    "    experiment_logger.info(f\"Summary plot of first 10 MC runs saved to: {multi_avg_x_path_svg} and {multi_avg_x_path_png}\")\n",
    "\n",
    "# Plot average free energy across all runs\n",
    "free_energy_plot_path_svg, free_energy_plot_path_png, final_mean, final_sem, final_std = plot_avg_free_energy(\n",
    "    free_energy_array,\n",
    "    directory,\n",
    "    color='C2'\n",
    ")\n",
    "experiment_logger.info(f\"Average free energy plot saved to: {free_energy_plot_path_svg} and {free_energy_plot_path_png}\")\n",
    "experiment_logger.info(f\"Final mean delta F = {final_mean}\")\n",
    "experiment_logger.info(f\"Final standard error delta F = {final_sem}\")\n",
    "experiment_logger.info(f\"Final std delta F = {final_std}\")\n",
    "\n",
    "# -------------------------\n",
    "# After the full experiment, save each MC run's sampled data to a CSV file and the configurations (sample[6] for each sample) are saved as an NPY file\n",
    "# -------------------------\n",
    "for i, mc_run in enumerate(mc_runs):\n",
    "    run_folder = os.path.join(mc_runs_directory, f\"run_{i+1:03d}\")\n",
    "    csv_filename = os.path.join(run_folder, 'sampled_data.csv')\n",
    "    print(f\"Saving sampled data to {csv_filename}\")\n",
    "    \n",
    "    try:\n",
    "        with open(csv_filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\n",
    "                \"cycle_number\",\n",
    "                \"energy_per_particle\",\n",
    "                \"density\",\n",
    "                \"pressure\",\n",
    "                \"box_size_x\",\n",
    "                \"box_size_y\",\n",
    "                \"particle_configuration\"\n",
    "            ])\n",
    "            \n",
    "            for data in mc_run.local_samples:\n",
    "                # Expecting that each sample is structured as:\n",
    "                # (cycle_number, energy_per_particle, density, pressure, box_size_x, box_size_y, particles)\n",
    "                cycle_number, energy_per_particle, density, pressure, box_size_x, box_size_y, particles = data\n",
    "                \n",
    "                # Convert the particle configuration to a NumPy array and flatten it.\n",
    "                particles_flat = np.array(particles).flatten()\n",
    "                \n",
    "                writer.writerow([\n",
    "                    cycle_number,\n",
    "                    energy_per_particle,\n",
    "                    density,\n",
    "                    pressure,\n",
    "                    box_size_x,\n",
    "                    box_size_y,\n",
    "                    particles_flat.tolist()  # storing as a JSON-like list in the CSV\n",
    "                ])\n",
    "        experiment_logger.info(f\"MC run {i+1} sampled data successfully saved to {csv_filename}\")\n",
    "    except Exception as e:\n",
    "        experiment_logger.error(f\"Error: Failed to save sampled data for MC run {i+1} to {csv_filename}. Error: {e}\")\n",
    "\n",
    "    # Extract and save local samples configurations\n",
    "    configs = np.array([sample[6] for sample in mc_run.local_samples])\n",
    "    configs_filepath = os.path.join(run_folder, \"mc_run_configs.npy\")\n",
    "    np.save(configs_filepath, configs)\n",
    "    experiment_logger.info(f\"MC run {i+1} local config data saved to: {configs_filepath}\")\n",
    "\n",
    "    # Extract and save testing samples configurations\n",
    "    testing_configs = np.array(mc_run.testing_samples)\n",
    "    testing_configs_filepath = os.path.join(run_folder, \"mc_run_testing_configs.npy\") \n",
    "    np.save(testing_configs_filepath, testing_configs)\n",
    "    experiment_logger.info(f\"MC run {i+1} testing config data saved to: {testing_configs_filepath}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
