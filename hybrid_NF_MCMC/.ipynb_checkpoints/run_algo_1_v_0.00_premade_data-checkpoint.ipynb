{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from scipy.spatial.distance import squareform, cdist\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from utils import (get_project_root, setup_logger, get_dataloader, calculate_well_statistics, \n",
    "                  set_icl_color_cycle, get_icl_heatmap_cmap, plot_loss, \n",
    "                  plot_frequency_heatmap, generate_samples, \n",
    "                  calculate_pair_correlation, plot_pair_correlation, save_rdf_data, plot_acceptance_rate,\n",
    "                  plot_avg_free_energy, plot_well_statistics, plot_avg_x_coordinate,\n",
    "                  plot_multiple_avg_x_coordinates)\n",
    "import argparse\n",
    "import csv\n",
    "\n",
    "set_icl_color_cycle()\n",
    "cmap_div = get_icl_heatmap_cmap(\"diverging\")\n",
    "\n",
    "# importing the NF and MC modules\n",
    "project_root = get_project_root()\n",
    "nf_path = os.path.join(project_root, \"NF\")\n",
    "sys.path.append(nf_path)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "import normflows as NF\n",
    "import MCMC as MC\n",
    "\n",
    "# EXPERIMENT VARIABLES\n",
    "NUM_MC_RUNS = 10        # how many mc run running in 'parallel'\n",
    "MASTER_SEED = 42        # master seed to give seeds for each mc run to run differently\n",
    "NUM_PARTICLES = 3\n",
    "NUM_DIM = 2\n",
    "NUM_TRAINING_CYCLES = 0\n",
    "\n",
    "# mc needed params\n",
    "TEMP = 1.0\n",
    "RHO = 0.03\n",
    "ASPECT_RATIO = 1.0\n",
    "VISUALISE = True\n",
    "CHECKING = True\n",
    "EQUILIBRATION_STEPS = 5000\n",
    "NUM_WELLS = 2\n",
    "V0_LIST = [-10.0,-10.0]              # depth of wells\n",
    "R0 = 1.2                # radius of bottom of well\n",
    "K_VAL = 15              # steepness of well slopes\n",
    "HALF_BOX = ((NUM_PARTICLES/RHO)**(1/NUM_DIM))/2\n",
    "INITIAL_MAX_DISPLACEMENT = 0.65\n",
    "SAMPLING_FREQUENCY = 150\n",
    "ADJUSTING_FREQUENCY = 5000\n",
    "\n",
    "# nf needed params\n",
    "INITIAL_TRAINING_NUM_SAMPLES = 102400\n",
    "BATCH_SIZE = 512\n",
    "K = 15\n",
    "EPOCHS = 100\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 0\n",
    "HIDDEN_UNITS = 256\n",
    "NUM_BINS = 32\n",
    "N_BLOCKS = 8\n",
    "\n",
    "# post training runs parameters\n",
    "TESTING = True\n",
    "BIG_MOVE_ATTEMPTS = 1000\n",
    "BIG_MOVE_INTERVAL = 1000\n",
    "NUM_SAMPLES_FOR_ANALYSIS = BIG_MOVE_ATTEMPTS * NUM_MC_RUNS\n",
    "\n",
    "experiment_name = 'algo_1_premade_102400_samples_dV_0.0_samp_fr_150'\n",
    "\n",
    "# result saving\n",
    "# parser = argparse.ArgumentParser(add_help=False)\n",
    "# parser.add_argument(\"--experiment_id\", type=str, required=True, help=\"Unique identifier for the experiment\")\n",
    "# args, _ = parser.parse_known_args()\n",
    "# experiment_name = args.experiment_id\n",
    "\n",
    "directory = os.path.join(get_project_root(), \"results\", experiment_name)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "# Setup a global experiment logger that logs to a file in the experiment directory.\n",
    "experiment_log_file = os.path.join(directory, \"experiment.log\")\n",
    "experiment_logger = setup_logger(\"experiment\", experiment_log_file, file_level=logging.DEBUG, stream_level=logging.INFO)\n",
    "experiment_logger.info(\"half box is: \" + str(HALF_BOX))\n",
    "experiment_logger.info(f\"Directory created at: {directory}\")\n",
    "\n",
    "mc_runs_directory = os.path.join(directory, \"mc_runs\")\n",
    "os.makedirs(mc_runs_directory, exist_ok=True)\n",
    "training_rounds_directory = os.path.join(directory, \"training_rounds\")\n",
    "os.makedirs(training_rounds_directory, exist_ok=True)\n",
    "\n",
    "# Save experiment parameters to a JSON file in the parent experiment directory\n",
    "experiment_params = {\n",
    "    \"NUM_MC_RUNS\": NUM_MC_RUNS,\n",
    "    \"MASTER_SEED\": MASTER_SEED,\n",
    "    \"NUM_PARTICLES\": NUM_PARTICLES,\n",
    "    \"NUM_DIM\": NUM_DIM,\n",
    "    \"NUM_TRAINING_CYCLES\": NUM_TRAINING_CYCLES,\n",
    "    \"INITIAL_TRAINING_NUM_SAMPLES\": INITIAL_TRAINING_NUM_SAMPLES,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"K\": K,\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"LR\": LR,\n",
    "    \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
    "    \"HIDDEN_UNITS\": HIDDEN_UNITS,\n",
    "    \"NUM_BINS\": NUM_BINS,\n",
    "    \"N_BLOCKS\": N_BLOCKS,\n",
    "    \"TEMP\": TEMP,\n",
    "    \"RHO\": RHO,\n",
    "    \"ASPECT_RATIO\": ASPECT_RATIO,\n",
    "    \"VISUALISE\": VISUALISE,\n",
    "    \"CHECKING\": CHECKING,\n",
    "    \"EQUILIBRATION_STEPS\": EQUILIBRATION_STEPS,\n",
    "    \"NUM_WELLS\": NUM_WELLS,\n",
    "    \"V0_LIST\": V0_LIST,\n",
    "    \"R0\": R0,\n",
    "    \"K_VAL\": K_VAL,\n",
    "    \"HALF_BOX\": HALF_BOX,\n",
    "    \"INITIAL_MAX_DISPLACEMENT\": INITIAL_MAX_DISPLACEMENT,\n",
    "    \"SAMPLING_FREQUENCY\": SAMPLING_FREQUENCY,\n",
    "    \"ADJUSTING_FREQUENCY\": ADJUSTING_FREQUENCY,\n",
    "    \"TESTING\": TESTING,\n",
    "    \"BIG_MOVE_ATTEMPTS\": BIG_MOVE_ATTEMPTS,\n",
    "    \"BIG_MOVE_INTERVAL\": BIG_MOVE_INTERVAL,\n",
    "    \"NUM_SAMPLES_FOR_ANALYSIS\": NUM_SAMPLES_FOR_ANALYSIS\n",
    "}\n",
    "json_file_path = os.path.join(directory, \"params.json\")\n",
    "with open(json_file_path, \"w\") as f:\n",
    "    json.dump(experiment_params, f, indent=4)\n",
    "experiment_logger.info(f\"Experiment parameters saved to {json_file_path}\")\n",
    "\n",
    "# initialisation of experiment - sets up the monte carlo runs and equilibrates and collects samples\n",
    "mc_runs = []  # list to store each Monte Carlo simulation instance\n",
    "for i in range(NUM_MC_RUNS):\n",
    "    seed = i + MASTER_SEED\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create a dedicated folder and logger for this Monte Carlo run.\n",
    "    run_dir = os.path.join(mc_runs_directory, f\"run_{i+1:03d}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    mc_log_file = os.path.join(run_dir, \"mc_run.log\")\n",
    "    mc_logger = setup_logger(f\"MC_run_{i+1:03d}\", mc_log_file, file_level=logging.DEBUG, stream_level=logging.WARNING)\n",
    "\n",
    "    # even i runs initialise left and odd initialise right\n",
    "    if i % 2 == 0:\n",
    "        particles, sim_box = MC.initialise_low_left(\n",
    "                num_particles=NUM_PARTICLES,\n",
    "                rho=RHO,\n",
    "                aspect_ratio=ASPECT_RATIO,\n",
    "                visualise=False,\n",
    "                checking=False\n",
    "            )\n",
    "        experiment_logger.info(f\"run {i} starts in left\")\n",
    "    else:\n",
    "        particles, sim_box = MC.initialise_low_right(\n",
    "                num_particles=NUM_PARTICLES,\n",
    "                rho=RHO,\n",
    "                aspect_ratio=ASPECT_RATIO,\n",
    "                visualise=False,\n",
    "                checking=False\n",
    "            )\n",
    "        experiment_logger.info(f\"run {i} starts in right\")\n",
    "        \n",
    "    mc_run = MC.MonteCarlo(\n",
    "        particles=particles,\n",
    "        sim_box=sim_box,\n",
    "        temperature=TEMP,\n",
    "        num_particles=NUM_PARTICLES,\n",
    "        num_wells=NUM_WELLS,    # Pass number of wells\n",
    "        V0_list=V0_LIST,                  # Pass well depth\n",
    "        r0=R0,\n",
    "        k=K_VAL,\n",
    "        initial_max_displacement=INITIAL_MAX_DISPLACEMENT,  # Adjust as needed\n",
    "        target_acceptance=0.5,\n",
    "        timing=False,\n",
    "        checking=CHECKING,\n",
    "        logger=mc_logger,       # Each run now logs to its own file\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    mc_runs.append(mc_run)\n",
    "    experiment_logger.info(f\"Monte Carlo run {i} initialised and stored\")\n",
    "\n",
    "# Plot the potential wells for visualization\n",
    "MC.visualise.plot_potential(\n",
    "    box_siz_x=sim_box.box_size_x,\n",
    "    box_size_y=sim_box.box_size_y,\n",
    "    V0_list=V0_LIST,\n",
    "    r0=R0,\n",
    "    k=K_VAL,\n",
    "    num_wells=NUM_WELLS,\n",
    "    output_path=directory\n",
    ")\n",
    "\n",
    "experiment_logger.info(\"All Monte Carlo runs initialised\")\n",
    "experiment_logger.info(\"list of run instances: \" + str(mc_runs))\n",
    "\n",
    "# equilibration of each mc run\n",
    "for mc_run in mc_runs:\n",
    "    for step in range(1, EQUILIBRATION_STEPS + 1):\n",
    "        mc_run.particle_displacement()\n",
    "        if step % ADJUSTING_FREQUENCY == 0:\n",
    "            mc_run.adjust_displacement()\n",
    "        if step % SAMPLING_FREQUENCY == 0:\n",
    "            sample = mc_run.sample(step)\n",
    "            mc_run.local_samples.append(sample)\n",
    "    \n",
    "# Plot and save the equilibrated configuration for each MC run\n",
    "for i, mc_run in enumerate(mc_runs):\n",
    "    run_dir = os.path.join(mc_runs_directory, f\"run_{i+1:03d}\")\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    current_config = mc_run.particles\n",
    "    plt.scatter(current_config[:, 0], current_config[:, 1], alpha=0.6)\n",
    "    plt.xlim(0, 2*HALF_BOX)\n",
    "    plt.ylim(0, 2*HALF_BOX)\n",
    "    plt.xlabel(r'$x$')\n",
    "    plt.ylabel(r'$y$') \n",
    "    plt.title(f'Equilibrated Configuration - MC Run {i+1}')\n",
    "    \n",
    "    # Save plot to the MC run's directory\n",
    "    config_plot_path = os.path.join(run_dir, f\"equilibrated_config.png\")\n",
    "    fig.savefig(config_plot_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    mc_run.logger.info(f\"Equilibrated configuration plot saved to: {config_plot_path}\")\n",
    "\n",
    "# Initialize MCMC step counter right after equilibration\n",
    "total_mcmc_steps = 0\n",
    "\n",
    "# Initialize acceptance rate tracking at the very beginning\n",
    "p_acc_history = [0.0]  # Initial acceptance rate is 0\n",
    "mcmc_steps_history = [0]  # Start from MCMC step 0\n",
    "big_move_attempts = 0\n",
    "big_move_accepts = 0\n",
    "\n",
    "# prepping initial samples as for dataloader\n",
    "enable_cuda = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n",
    "experiment_logger.info('device is: ' + str(device))\n",
    "\n",
    "num_samples = 102400\n",
    "data_path = \"/home/n2401517d/my_workspace/flow_state/NF/data/samples_N3_rho_0.03.npz\"\n",
    "\n",
    "# IMPORT DATA\n",
    "npz_data = np.load(data_path)\n",
    "df = npz_data[npz_data.files[0]]\n",
    "# Get unique samples\n",
    "unique_data = np.unique(df, axis=0)\n",
    "available_samples = unique_data.shape[0]\n",
    "print(\"\\nTotal unique samples available:\", available_samples)\n",
    "\n",
    "# Use the entire dataset if num_samples is not provided\n",
    "if num_samples is None:\n",
    "        num_samples = available_samples\n",
    "        print(f\"No num_samples specified. Using all available samples: {num_samples}\")\n",
    "elif num_samples > available_samples:\n",
    "        raise ValueError(f\"Error: Requested number of samples ({num_samples}) exceeds available unique samples ({available_samples}).\")\n",
    "\n",
    "# Randomly select the desired number of samples\n",
    "indices = np.random.choice(available_samples, num_samples, replace=False)\n",
    "global_samples_nf = unique_data[indices]\n",
    "global_samples_nf = global_samples_nf.reshape(num_samples, NUM_PARTICLES * NUM_DIM)\n",
    "print(\"Flattened global_samples_nf shape:\", global_samples_nf.shape)\n",
    "print('Data Loaded!')\n",
    "\n",
    "unique_data = np.unique(global_samples_nf, axis=0)\n",
    "experiment_logger.info(\"Total unique samples: \" + str(unique_data.shape[0]))\n",
    "\n",
    "dataloader = get_dataloader(global_samples_nf, NUM_PARTICLES, NUM_DIM, device, batch_size=BATCH_SIZE, shuffle=True)\n",
    "experiment_logger.info(\"DataLoader details:\")\n",
    "experiment_logger.info(\"Dataset size: \" + str(len(dataloader.dataset)))\n",
    "experiment_logger.info(\"Number of batches: \" + str(len(dataloader)))\n",
    "first_batch = next(iter(dataloader))\n",
    "experiment_logger.info(\"Shape of data in first batch: \" + str(first_batch[0].shape))\n",
    "\n",
    "# initialise the normalizing flow model\n",
    "bound = HALF_BOX\n",
    "base = NF.Energy.UniformParticle(NUM_PARTICLES, NUM_DIM, bound, device=device)\n",
    "# target = NF.Energy.SimpleLJ(NUM_PARTICLES * NUM_DIM, NUM_PARTICLES, 1, bound)    # not used for now\n",
    "# K = 15 - now defined in the argument of function\n",
    "flow_layers = []\n",
    "for i in range(K):\n",
    "    flow_layers += [NF.flows.CircularCoupledRationalQuadraticSpline(NUM_PARTICLES * NUM_DIM, NUM_BINS, HIDDEN_UNITS,\n",
    "                    range(NUM_PARTICLES * NUM_DIM), num_bins=NUM_BINS, tail_bound=bound)]\n",
    "model = NF.NormalizingFlow(base, flow_layers).to(device)\n",
    "experiment_logger.info(f'Model prepared with {NUM_PARTICLES} particles and {NUM_DIM} dimensions!')\n",
    "\n",
    "# Create a dedicated folder and logger for the first training round of the normalizing flow.\n",
    "nf_training_dir = os.path.join(training_rounds_directory, \"initial_training_round\")\n",
    "os.makedirs(nf_training_dir, exist_ok=True)\n",
    "nf_log_file = os.path.join(nf_training_dir, \"nf_training_round.log\")\n",
    "nf_logger = setup_logger(\"NF_training_round\", nf_log_file, file_level=logging.DEBUG, stream_level=logging.WARNING)\n",
    "\n",
    "# train the normalizing flow on initial training batch\n",
    "loss_hist = np.array([])\n",
    "loss_epoch = []\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "experiment_logger.info(f'Optimizer prepared with learning rate: {LR}')\n",
    "data_save = []\n",
    "\n",
    "for it in trange(EPOCHS, desc=\"Training Progress\"):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # Compute loss\n",
    "        # loss,z = model.reverse_kld(num_samples)\n",
    "        loss = model.forward_kld(batch[0].to(device))\n",
    "        # Do backprop and optimizer step\n",
    "        # if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            experiment_logger.warning(\"Warning: Loss is NaN or Inf. Skipping this batch.\")\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "        # Log loss\n",
    "        loss_hist = np.append(loss_hist, loss.to('cpu').data.numpy())\n",
    "        epoch_loss += loss.item()\n",
    "    loss_epoch.append(epoch_loss / len(dataloader))\n",
    "    nf_logger.info(f'Epoch {it+1}/{EPOCHS}, Loss: {loss_epoch[-1]:.4f}')\n",
    "\n",
    "# Plot loss\n",
    "loss_plot_path_svg, loss_plot_path_png = plot_loss(loss_epoch, nf_training_dir)\n",
    "nf_logger.info(f\"Loss plot saved successfully to: {loss_plot_path_png}\")\n",
    "\n",
    "model_path = os.path.join(nf_training_dir, \"initial_model_circularspline_res_dense.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "nf_logger.info(f\"Model saved successfully to: {model_path}\")\n",
    "\n",
    "# change to eval mode to test the NF model\n",
    "model.eval()\n",
    "samples_for_mc = []\n",
    "z = model.sample(NUM_MC_RUNS)\n",
    "z_ = z.reshape(-1, NUM_PARTICLES, NUM_DIM)  # Reshape as required\n",
    "z_ = z_.to('cpu').detach().numpy()  # Move to CPU and convert to NumPy\n",
    "z_ = z_ + HALF_BOX  # Add HALF_BOX to each element in the samples\n",
    "print(z_)\n",
    "\n",
    "# Generate samples\n",
    "a_ = generate_samples(model, NUM_PARTICLES, NUM_DIM, \n",
    "                     n_iterations=NUM_SAMPLES_FOR_ANALYSIS // 5000 + 1, \n",
    "                     samples_per_iteration=5000)\n",
    "a_ = a_ + HALF_BOX  # Add HALF_BOX to each element in the samples\n",
    "print(a_.shape)\n",
    "samples_path = os.path.join(nf_training_dir, \"samples.npy\")\n",
    "np.save(samples_path, a_)\n",
    "final_model_samples = a_\n",
    "\n",
    "# make heatmap\n",
    "heatmap_path_svg, heatmap_path_png = plot_frequency_heatmap(a_- HALF_BOX, nf_training_dir, cmap_div, HALF_BOX)\n",
    "nf_logger.info(f\"Frequency heatmap saved successfully to: {heatmap_path_png}\")\n",
    "\n",
    "# Calculate and plot pair correlation function\n",
    "r_vals, g_r = calculate_pair_correlation(a_ - HALF_BOX, NUM_PARTICLES, HALF_BOX, dr=HALF_BOX/50)\n",
    "pair_corr_path_svg, pair_corr_path_png = plot_pair_correlation(\n",
    "    r_vals, \n",
    "    g_r, \n",
    "    nf_training_dir + \"/\"\n",
    ")\n",
    "print(f\"Pair correlation function plot saved successfully to: {pair_corr_path_svg} and {pair_corr_path_png}\")\n",
    "\n",
    "# Add another data point showing 0 acceptance at the end of training\n",
    "p_acc_history.append(0.0)\n",
    "mcmc_steps_history.append(total_mcmc_steps)\n",
    "\n",
    "# -------------------------\n",
    "# After the full experiment, running the mc_runs with the samples from the final trained model\n",
    "# -------------------------\n",
    "trained_nf_model = model  # this is your trained normalizing flow model\n",
    "\n",
    "# For each Monte Carlo simulation instance, assign the trained model:\n",
    "for mc_run in mc_runs:\n",
    "    mc_run.set_nf_model(trained_nf_model)\n",
    "\n",
    "if TESTING:\n",
    "    test_configs = final_model_samples  # configurations produced by the final model for big move suggestions\n",
    "    \n",
    "    free_energy_array = []\n",
    "\n",
    "    # Testing phase: for each MC run, run displacement steps and suggest a big move with a unique config for each attempt\n",
    "    for run_idx, mc_run in enumerate(mc_runs):\n",
    "        for attempt in range(BIG_MOVE_ATTEMPTS):\n",
    "            # Run BIG_MOVE_INTERVAL displacement steps and collect testing samples\n",
    "            for step in range(1, BIG_MOVE_INTERVAL + 1):\n",
    "                mc_run.particle_displacement()\n",
    "                total_mcmc_steps += 1\n",
    "                if step % ADJUSTING_FREQUENCY == 0:\n",
    "                    mc_run.adjust_displacement()\n",
    "                if step % SAMPLING_FREQUENCY == 0:\n",
    "                    sample = mc_run.sample(step)\n",
    "                    mc_run.local_samples.append(sample)\n",
    "                    mc_run.testing_samples.append(sample[6])  # store configuration (in MC box coordinates)\n",
    "            \n",
    "            # Each MC run receives a unique configuration for a big move suggestion\n",
    "            test_config = test_configs[attempt * len(mc_runs) + run_idx]\n",
    "            # Assume nf_big_move returns a boolean indicating whether the big move was accepted\n",
    "            accepted = mc_run.nf_big_move(test_config)\n",
    "            big_move_attempts += 1\n",
    "            if accepted:\n",
    "                big_move_accepts += 1\n",
    "            \n",
    "            # Initialize testing move counters if they don't exist yet\n",
    "            if not hasattr(mc_run, 'test_moves_attempted'):\n",
    "                mc_run.test_moves_attempted = 0\n",
    "                mc_run.test_moves_accepted = 0\n",
    "            \n",
    "            # Update testing counters for this simulation run\n",
    "            mc_run.test_moves_attempted += 1\n",
    "            if accepted:\n",
    "                mc_run.test_moves_accepted += 1\n",
    "            \n",
    "            # Calculate and print the acceptance probability for this simulation run so far\n",
    "            p_accept = mc_run.test_moves_accepted / mc_run.test_moves_attempted\n",
    "            \n",
    "            # Track global acceptance rate after each big move attempt\n",
    "            current_global_p_acc = big_move_accepts / big_move_attempts\n",
    "            p_acc_history.append(current_global_p_acc)\n",
    "            mcmc_steps_history.append(total_mcmc_steps)\n",
    "                \n",
    "            experiment_logger.info(f\"Step {total_mcmc_steps}, Run {run_idx+1}, Attempt {attempt+1}: \"\n",
    "                                    f\"p_accept = {current_global_p_acc:.4f} ({big_move_accepts}/{big_move_attempts})\")\n",
    "\n",
    "        # Print summary after all attempts for this run are complete\n",
    "        print(f\"Testing Summary - Simulation run {run_idx+1}: Final p_accept = {p_accept:.4f} ({mc_run.test_moves_accepted}/{mc_run.test_moves_attempted} moves accepted)\")\n",
    "\n",
    "    # Plot the acceptance rate history\n",
    "    acc_plot_path_svg, acc_plot_path_png = plot_acceptance_rate(\n",
    "        p_acc_history, \n",
    "        directory, \n",
    "        x_values=mcmc_steps_history, \n",
    "        xlabel='MCMC Steps',\n",
    "        base_filename=\"nf_acceptance_rate\",\n",
    "        color='C2'\n",
    "    )\n",
    "    experiment_logger.info(f\"Acceptance rate plot saved to: {acc_plot_path_png}\")\n",
    "    \n",
    "    # Save the acceptance rate data to a CSV file\n",
    "    acceptance_data_path = os.path.join(directory, \"acceptance_rate_data.csv\")\n",
    "    with open(acceptance_data_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['MCMC_Steps', 'Acceptance_Rate'])\n",
    "        for steps, acc_rate in zip(mcmc_steps_history, p_acc_history):\n",
    "            writer.writerow([steps, acc_rate])\n",
    "    experiment_logger.info(f\"Acceptance rate data saved to: {acceptance_data_path}\")\n",
    "\n",
    "for run_idx, mc_run in enumerate(mc_runs):\n",
    "    if mc_run.testing_samples:\n",
    "        testing_configs = np.array(mc_run.testing_samples)  # shape: (num_samples, num_particles, 2)\n",
    "        testing_configs = testing_configs[:]\n",
    "        \n",
    "        # Compute well statistics\n",
    "        avg_x_values, p_a_values, p_b_values, deltaF_normalized_values, runs = calculate_well_statistics(\n",
    "            testing_configs, 0, HALF_BOX, R0\n",
    "        )\n",
    "\n",
    "        # Add this run's free energy data to the array\n",
    "        free_energy_array.append(deltaF_normalized_values)\n",
    "\n",
    "        # Plot well statistics\n",
    "        run_folder = os.path.join(mc_runs_directory, f\"run_{run_idx+1:03d}\")\n",
    "        well_stats_path_svg, well_stats_path_png = plot_well_statistics(\n",
    "            avg_x_values, \n",
    "            p_a_values, \n",
    "            p_b_values, \n",
    "            deltaF_normalized_values, \n",
    "            runs, \n",
    "            HALF_BOX,\n",
    "            run_folder\n",
    "        )\n",
    "        \n",
    "        # Plot average x coordinate for individual particles\n",
    "        avg_x_path_svg, avg_x_path_png = plot_avg_x_coordinate(\n",
    "            testing_configs,\n",
    "            run_folder,\n",
    "            run_idx+1\n",
    "        )\n",
    "\n",
    "# Plot summary of the first 10 MC runs if there are at least 10\n",
    "if len(mc_runs) >= 10:\n",
    "    multi_avg_x_path_svg, multi_avg_x_path_png = plot_multiple_avg_x_coordinates(\n",
    "        mc_runs,\n",
    "        directory\n",
    "    )\n",
    "    experiment_logger.info(f\"Summary plot of first 10 MC runs saved to: {multi_avg_x_path_svg} and {multi_avg_x_path_png}\")\n",
    "\n",
    "# Plot average free energy across all runs\n",
    "free_energy_plot_path_svg, free_energy_plot_path_png, final_mean, final_sem, final_std = plot_avg_free_energy(\n",
    "    free_energy_array,\n",
    "    directory,\n",
    "    color='C2'\n",
    ")\n",
    "experiment_logger.info(f\"Average free energy plot saved to: {free_energy_plot_path_svg} and {free_energy_plot_path_png}\")\n",
    "experiment_logger.info(f\"Final mean delta F = {final_mean}\")\n",
    "experiment_logger.info(f\"Final standard error delta F = {final_sem}\")\n",
    "experiment_logger.info(f\"Final std delta F = {final_std}\")\n",
    "\n",
    "# -------------------------\n",
    "# After the full experiment, save each MC run's sampled data to a CSV file and the configurations (sample[6] for each sample) are saved as an NPY file\n",
    "# -------------------------\n",
    "for i, mc_run in enumerate(mc_runs):\n",
    "    run_folder = os.path.join(mc_runs_directory, f\"run_{i+1:03d}\")\n",
    "    csv_filename = os.path.join(run_folder, 'sampled_data.csv')\n",
    "    print(f\"Saving sampled data to {csv_filename}\")\n",
    "    \n",
    "    try:\n",
    "        with open(csv_filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\n",
    "                \"cycle_number\",\n",
    "                \"energy_per_particle\",\n",
    "                \"density\",\n",
    "                \"pressure\",\n",
    "                \"box_size_x\",\n",
    "                \"box_size_y\",\n",
    "                \"particle_configuration\"\n",
    "            ])\n",
    "            \n",
    "            for data in mc_run.local_samples:\n",
    "                # Expecting that each sample is structured as:\n",
    "                # (cycle_number, energy_per_particle, density, pressure, box_size_x, box_size_y, particles)\n",
    "                cycle_number, energy_per_particle, density, pressure, box_size_x, box_size_y, particles = data\n",
    "                \n",
    "                # Convert the particle configuration to a NumPy array and flatten it.\n",
    "                particles_flat = np.array(particles).flatten()\n",
    "                \n",
    "                writer.writerow([\n",
    "                    cycle_number,\n",
    "                    energy_per_particle,\n",
    "                    density,\n",
    "                    pressure,\n",
    "                    box_size_x,\n",
    "                    box_size_y,\n",
    "                    particles_flat.tolist()  # storing as a JSON-like list in the CSV\n",
    "                ])\n",
    "        experiment_logger.info(f\"MC run {i+1} sampled data successfully saved to {csv_filename}\")\n",
    "    except Exception as e:\n",
    "        experiment_logger.error(f\"Error: Failed to save sampled data for MC run {i+1} to {csv_filename}. Error: {e}\")\n",
    "\n",
    "    # Extract and save local samples configurations\n",
    "    configs = np.array([sample[6] for sample in mc_run.local_samples])\n",
    "    configs_filepath = os.path.join(run_folder, \"mc_run_configs.npy\")\n",
    "    np.save(configs_filepath, configs)\n",
    "    experiment_logger.info(f\"MC run {i+1} local config data saved to: {configs_filepath}\")\n",
    "\n",
    "    # Extract and save testing samples configurations\n",
    "    testing_configs = np.array(mc_run.testing_samples)\n",
    "    testing_configs_filepath = os.path.join(run_folder, \"mc_run_testing_configs.npy\") \n",
    "    np.save(testing_configs_filepath, testing_configs)\n",
    "    experiment_logger.info(f\"MC run {i+1} testing config data saved to: {testing_configs_filepath}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
